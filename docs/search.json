[{"path":[]},{"path":"https://lin380.github.io/tadr/articles/guide_1.html","id":"apply-for-a-developer-account","dir":"Articles","previous_headings":"","what":"Apply for a developer account","title":"1. Twitter API Developer Setup","text":"must Twitter account https://developer.twitter.com/en Choose ‘Student’ use Twitter API/ Data?1 planning analyze Twitter data? Yes. Add general description.2 app use Tweet, Retweet, Like, Follow, Direct Message functionality? . plan display Tweets aggregate data Twitter content outside Twitter? Yes. Describe data used research project (details like) aggregated (share full tweet content outside Twitter)3 product, service, analysis make Twitter content derived information available government entity? . Submit application verify email","code":""},{"path":"https://lin380.github.io/tadr/articles/guide_1.html","id":"create-a-project","dir":"Articles","previous_headings":"","what":"Create a project","title":"1. Twitter API Developer Setup","text":"Name project (Example “Language use”) Use case: “Student” Project description: Use description “planning analyze Twitter data?” Developer application. App name: give name GitHub Project, “project_”, mine “project_francom”. Copy API Key, API Key Secret, Bearer Token store safe accessible place (’ll need later.) Enter “CALLBACK URLS” http://127.0.0.1:1410 Enter “WEBSITE URL” twitter profile page (e.g. https://twitter.com/jrad_cole)","code":""},{"path":"https://lin380.github.io/tadr/articles/guide_1.html","id":"authenticate-with-r","dir":"Articles","previous_headings":"","what":"Authenticate with R","title":"1. Twitter API Developer Setup","text":"Note need complete following steps local machine. create authentication token RStudio Cloud. Refer guide install R RStudio computer. Open .R script save _twitter_auth.R plan pushing project GitHub: add _twitter_auth.R list Continue following steps: Load rtweet package pass credentials create_token() function. now token saved R Project environment use. want use token RStudio Cloud, need first save token .rds file. saveRDS(token, file = \"~/Desktop/token.rds\") upload RStudio Cloud project want interface Twitter API. use token, read token.rds file assign object. token = readRDS(\"token.rds\") Use object name (token) calls rtweet functions (token = token). One final note, plan push project GitHub include name .rds token file .gitignore made public.","code":"library(rtweet) ## stored api keys  # (these are fake example values; replace with your own keys) app_name <- \"project_francom\" api_key <- \"afYS4vbIlPAj096E60c4W1fiK\" api_secret_key <- \"bI91kqnqFoNCrZFbsjAWHD4gJ91LQAhdCJXCj3yscfuULtNkuu\"  ## authenticate via web browser token <- create_token(   app = app_name,   consumer_key = api_key,   consumer_secret = api_secret_key)"},{"path":"https://lin380.github.io/tadr/articles/guide_2.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"2. Working with tidycensus and Twitter geolocation","text":"guide provide overview coordinate US Census demographic information Twitter status posts collected Twitter API include geolocation information (.e. latitude longitude coordinates).","code":""},{"path":"https://lin380.github.io/tadr/articles/guide_2.html","id":"access-to-the-us-census","dir":"Articles","previous_headings":"","what":"Access to the US Census","title":"2. Working with tidycensus and Twitter geolocation","text":"first step acquire US Census API Key. can following link. API key, now want add RStudio –want add R environment, script project directory. can run tidycensus::census_api_key() function use API key character vector first argument set install = TRUE. install = TRUE argument make API key available across RStudio sessions.","code":"tidycensus::census_api_key(key = \"<your-api-key-here>\", install = TRUE)"},{"path":"https://lin380.github.io/tadr/articles/guide_2.html","id":"query-us-census-data","dir":"Articles","previous_headings":"Access to the US Census","what":"Query US Census data","title":"2. Working with tidycensus and Twitter geolocation","text":"use tidycensus package (Walker Herman 2022) query US Census. particular look querying American Community Survey (ACS) data rolling survey 5 years (“acs5”). make easier find codes various variables survey provides, can acess variables table load_variables() function. Assigning object calling View() allows us peruse variables search filter tool RStudio, seen Figure 1. Figure 1: ACS 5 survey variable codes descriptions use get_acs() function pull demographic information. can specify state(s) query type geography (block, tract, county, state) want along variable(s) want. addition can pull geometry geography can geographic limits geographies. Let’s use variable B19013_001 median household income past 12 months counties Arizona. also request geometry counties graphic visualization. get_acs() defaults 2019 ACS 5-year sample. Let’s take look structure dataset returned. dataset 15 observations corresponding 15 counties Arizona. columns include GEOID, NAME (county name case), variable (median income), estimate (median income values), moe (margin error sample), geometry. Let’s visualize median income counties Arizona ggplot() geom_sf() function.  can also query multiple states variables. Let’s look education B07009 category. sample total B07009_001 estimate sample less high school education B07009_002. dataset columns , yet double observations. due fact variable default added tidy format seen . variables want may want compare particular variable variable’s total estimate get percentage, often better data wide format. can run similar query add argument output = \"wide\". dataset number observations, additional columns. estimate (E) margin error (M) appear variable. Now multiple variables education wide format can use total estimate (B07009_001E) less high school estimate (B07009_002E) create percentage county’s population less high school education. use data manipulation skills . use mutate() calculate percentage add data frame. can visualize information ggplot().","code":"acs_5 <-    load_variables(year = 2019, # select last year of 5-year summary                  dataset = \"acs5\", # select the dataset                  cache = TRUE) # cache the results (to speed subsequent queries)  View(acs_5) # view in RStudio editor pane az_county_sf <-    get_acs(state = \"AZ\", # state           geography = \"county\", # census geography           variables = \"B19013_001\", # median income           geometry = TRUE) # get simple features (sf) geography glimpse(az_county_sf)  # preview ## Rows: 15 ## Columns: 6 ## $ GEOID    <chr> \"04027\", \"04001\", \"04017\", \"04013\", \"04012\", \"04005\", \"04021\"… ## $ NAME     <chr> \"Yuma County, Arizona\", \"Apache County, Arizona\", \"Navajo Cou… ## $ variable <chr> \"B19013_001\", \"B19013_001\", \"B19013_001\", \"B19013_001\", \"B190… ## $ estimate <dbl> 48790, 33967, 43140, 67799, 34956, 59000, 60968, 46907, 47686… ## $ moe      <dbl> 1981, 2108, 2357, 444, 5708, 1900, 1223, 3169, 1355, 3071, 11… ## $ geometry <MULTIPOLYGON [°]> MULTIPOLYGON (((-115 32.5, ..., MULTIPOLYGON (((… az_county_sf %>%     slice_head(n = 5) az_county_sf %>%    ggplot(aes(fill = estimate)) + # mapping   geom_sf() + # plot the county geometries   theme_minimal() + # use a minimal ggplot theme   labs(title = \"Arizona: Median income by county\",         fill = \"Median income\") # labels az_county_sf <-    get_acs(state = \"AZ\", # state           geography = \"county\", # census geography           variables = c(\"B07009_001\", \"B07009_002\"), # edu: total, < high school           geometry = TRUE) # get simple features (sf) geography glimpse(az_county_sf)  # preview structure ## Rows: 30 ## Columns: 6 ## $ GEOID    <chr> \"04027\", \"04027\", \"04001\", \"04001\", \"04017\", \"04017\", \"04013\"… ## $ NAME     <chr> \"Yuma County, Arizona\", \"Yuma County, Arizona\", \"Apache Count… ## $ variable <chr> \"B07009_001\", \"B07009_002\", \"B07009_001\", \"B07009_002\", \"B070… ## $ estimate <dbl> 134749, 34280, 45607, 8459, 71918, 11620, 2952370, 342904, 16… ## $ moe      <dbl> 297, 1420, 109, 410, 76, 609, NA, 5371, 194, 473, 157, 600, 1… ## $ geometry <MULTIPOLYGON [°]> MULTIPOLYGON (((-115 32.5, ..., MULTIPOLYGON (((… az_county_sf %>%     slice_head(n = 5)  # preview data frame az_county_sf <-    get_acs(state = \"AZ\", # state           geography = \"county\", # census geography           variables = c(\"B07009_001\", \"B07009_002\"), # edu: total, < high school           geometry = TRUE, # get simple features (sf) geography           output = \"wide\") # get wide output glimpse(az_county_sf)  # preview structure ## Rows: 15 ## Columns: 7 ## $ GEOID       <chr> \"04027\", \"04001\", \"04017\", \"04013\", \"04012\", \"04005\", \"040… ## $ NAME        <chr> \"Yuma County, Arizona\", \"Apache County, Arizona\", \"Navajo … ## $ B07009_001E <dbl> 134749, 45607, 71918, 2952370, 16600, 84314, 311415, 39740… ## $ B07009_001M <dbl> 297, 109, 76, NA, 194, 157, 128, 39, 130, 105, 131, 123, 1… ## $ B07009_002E <dbl> 34280, 8459, 11620, 342904, 3245, 7597, 40308, 5063, 22045… ## $ B07009_002M <dbl> 1420, 410, 609, 5371, 473, 600, 1602, 577, 1325, 389, 913,… ## $ geometry    <MULTIPOLYGON [°]> MULTIPOLYGON (((-115 32.5, ..., MULTIPOLYGON … az_county_sf %>%     slice_head(n = 5)  # preview data frame az_county_sf <-    az_county_sf %>%    mutate(perc_less_hs = (B07009_002E / B07009_001E) * 100) # calculate percent of county with less than high school edu  az_county_sf %>%    select(!ends_with(\"M\")) %>% # filter out margin of error columns   slice_head(n = 5) # preview az_county_sf %>%    ggplot(aes(fill = perc_less_hs)) + # mapping   geom_sf() + # plot the county geometries   theme_minimal() + # use a minimal ggplot theme   labs(title = \"Arizona: Less than high school by county\",         subtitle = \"Percentage\",        fill = \"Less than high school\") # labels"},{"path":"https://lin380.github.io/tadr/articles/guide_2.html","id":"twitter","dir":"Articles","previous_headings":"","what":"Twitter","title":"2. Working with tidycensus and Twitter geolocation","text":"Now let’s look join information US Census Twitter status posts (tweets) geolocation information (.e. latitude longitude coordinates).","code":""},{"path":"https://lin380.github.io/tadr/articles/guide_2.html","id":"orientation","dir":"Articles","previous_headings":"Twitter","what":"Orientation","title":"2. Working with tidycensus and Twitter geolocation","text":"example, read dataset collected Twitter API via rtweet (Kearney 2020). collection includes various regionalisms (faucet, spigot, frying pan, skillet, pail, bucket, coke, pop, soda, guys, yall). focus terms “y’” (“yall”) “guys”. also remove tweets geolocation coordinate information. 3018 observations 6 variables. Now let’s take look data dictionary dataset. Table 1: Data diciontary Twitter US regionalisms dataset. Since geolocation coordinates, can map using ggplot() base map map_data() function.  Trends towards “Y’” South eastern metropolitan areas. “guys” interspersed “Y’” degree appears common Mid-West northern states. overlap appears quite extensive states. Let’s look California particular explore can associate demographic variables tweets emanating tracts (sub-divisions counties) state.","code":"typ_df <-    read_csv(file = \"guide_2/data/derived/tusr_curated.csv\") %>% # read dataset   mutate(user_id = as.character(user_id), # convert to character          status_id = as.character(status_id)) %>%  # convert to character   filter(search_term %in% c(\"yall\", \"you guys\")) %>% # only keep you plural search terms   mutate(search_term = factor(search_term)) %>%  # make search term a factor   filter(lat != \"\") # remove tweets with no geolocation information  glimpse(typ_df) # preview ## Rows: 3,018 ## Columns: 6 ## $ user_id     <chr> \"29038175\", \"191434404\", \"1383299286373265408\", \"127876171… ## $ status_id   <chr> \"1452684447824887808\", \"1452684402715201536\", \"14526843251… ## $ search_term <fct> you guys, you guys, you guys, you guys, you guys, you guys… ## $ text        <chr> \"@WingfieldNFL @ckparrot Can you guys tell Grier and Co, s… ## $ lat         <dbl> 41.8, 43.6, 45.1, 43.0, 49.9, 49.9, 41.8, 41.8, 41.8, 33.9… ## $ lng         <dbl> -71.4, -79.3, -93.3, -80.6, -97.2, -97.2, -72.8, -72.8, -7… states_map <- map_data(\"state\") # get US map of states  p <- ggplot() + # base plot   geom_polygon(data = states_map, # map data                aes(x = long, y = lat, group = group), fill = \"white\", color = \"black\") + # set background/ lines   labs(title = \"Tweets in the USA\", subtitle = \"Regional terms\") + # labels   theme_minimal()  p + # add to previous base plot   geom_point(data = typ_df, # tweet data with lat and lng coordinates and languages              aes(x = lng, y = lat, group = 1, color = search_term), # map lat and lng and color for language names              alpha = 1/4, size = 1.5) + # transparency and size of points   labs(color = \"Regional terms\") # labels"},{"path":"https://lin380.github.io/tadr/articles/guide_2.html","id":"us-census","dir":"Articles","previous_headings":"Twitter","what":"US Census","title":"2. Working with tidycensus and Twitter geolocation","text":"Let’s pull demographic estimates median income, example, tracts California. dataset 9129 observations corresponding number tracts California 6 columns –saw earlier single variable, tidy version ACS query. fun, let’s plot census information tract median income California.  Now let’s add appropriate CRS (coordinate reference system) scheme lng lat variables tweet dataset (typ_df). use st_as_sf() function . ‘sf’ stands spatial features object map lng lat coords set crs CRS ca_tract_sf object. now see data frame converted latitude longitude geometry vector. can now use st_join() function associate geolocation points tweet data demographic information US Census. combined tweets census data resulting 3018 observations. Notice, however, various observations NA values. tweets emanate within California. can eliminate dataset purposes . filtering tweets originating California now 344 observations.","code":"ca_tract_sf <-    get_acs(state = \"CA\", # state           geography = \"tract\", # census geography           variables = c(\"B19013_001\"), # median income           geometry = TRUE) # get simple features (sf) geography glimpse(ca_tract_sf)  # preview ACS tracts for CA ## Rows: 9,129 ## Columns: 6 ## $ GEOID    <chr> \"06077005127\", \"06077003406\", \"06077004402\", \"06077005124\", \"… ## $ NAME     <chr> \"Census Tract 51.27, San Joaquin County, California\", \"Census… ## $ variable <chr> \"B19013_001\", \"B19013_001\", \"B19013_001\", \"B19013_001\", \"B190… ## $ estimate <dbl> 87125, 33750, 65850, 54034, 31098, 55956, 54754, 46696, 88897… ## $ moe      <dbl> 15157, 6659, 8817, 6463, 6802, 14313, 6871, 9628, 13887, 1033… ## $ geometry <MULTIPOLYGON [°]> MULTIPOLYGON (((-121 37.8, ..., MULTIPOLYGON (((… ca_tract_sf %>%    ggplot(aes(fill = estimate)) + # mapping   geom_sf() + # plot the county geometries   theme_minimal() + # use a minimal ggplot theme   labs(title = \"California: Median income by tract\",         fill = \"Median income\") # labels typ_sf <-    typ_df %>% # original dataset   st_as_sf(coords = c(\"lng\", \"lat\"), # map x/long, y/lat values to coords            crs = st_crs(ca_tract_sf)) # align coordinate reference systems  glimpse(typ_sf) # preview ## Rows: 3,018 ## Columns: 5 ## $ user_id     <chr> \"29038175\", \"191434404\", \"1383299286373265408\", \"127876171… ## $ status_id   <chr> \"1452684447824887808\", \"1452684402715201536\", \"14526843251… ## $ search_term <fct> you guys, you guys, you guys, you guys, you guys, you guys… ## $ text        <chr> \"@WingfieldNFL @ckparrot Can you guys tell Grier and Co, s… ## $ geometry    <POINT [°]> POINT (-71.4 41.8), POINT (-79.3 43.6), POINT (-93.3… typ_census_sf <- st_join(typ_sf, ca_tract_sf)  # join tweets and census by geometry  glimpse(typ_census_sf)  # preview ## Rows: 3,018 ## Columns: 10 ## $ user_id     <chr> \"29038175\", \"191434404\", \"1383299286373265408\", \"127876171… ## $ status_id   <chr> \"1452684447824887808\", \"1452684402715201536\", \"14526843251… ## $ search_term <fct> you guys, you guys, you guys, you guys, you guys, you guys… ## $ text        <chr> \"@WingfieldNFL @ckparrot Can you guys tell Grier and Co, s… ## $ geometry    <POINT [°]> POINT (-71.4 41.8), POINT (-79.3 43.6), POINT (-93.3… ## $ GEOID       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"06037650502\", NA, NA,… ## $ NAME        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Census Tract 6505.02,… ## $ variable    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"B19013_001\", NA, NA, … ## $ estimate    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, 122240, NA, NA, NA, NA… ## $ moe         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, 13636, NA, NA, NA, NA,… typ_census_ca_sf <- typ_census_sf %>%     filter(GEOID != \"\")  # keep observations with GEOIDs  glimpse(typ_census_ca_sf)  # preview ## Rows: 344 ## Columns: 10 ## $ user_id     <chr> \"1087968900\", \"15723895\", \"38469445\", \"61715298\", \"4398000… ## $ status_id   <chr> \"1452683638881984512\", \"1452677571825467392\", \"14526762309… ## $ search_term <fct> you guys, you guys, you guys, you guys, you guys, you guys… ## $ text        <chr> \"@melissawisz @StevieLynnne @delanieewisz @wiszrealestate … ## $ geometry    <POINT [°]> POINT (-118 33.9), POINT (-118 34), POINT (-118 34),… ## $ GEOID       <chr> \"06037650502\", \"06037271803\", \"06037271803\", \"06059088801\"… ## $ NAME        <chr> \"Census Tract 6505.02, Los Angeles County, California\", \"C… ## $ variable    <chr> \"B19013_001\", \"B19013_001\", \"B19013_001\", \"B19013_001\", \"B… ## $ estimate    <dbl> 122240, 83226, 83226, 62273, 83226, 132950, 83226, 155074,… ## $ moe         <dbl> 13636, 6367, 6367, 10934, 6367, 27282, 6367, 96400, NA, 17…"},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/guide_3.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"3. Creating R Markdown websites","text":"…coming soon…","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"1. Literate programming I","text":"Recipe introduce concept Literate Programming describe implement concept R Markdown. provide demonstration features R Markdown describe main structural characteristics R Markdown document help get running!","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"literate-programming","dir":"Articles","previous_headings":"","what":"Literate Programming","title":"1. Literate programming I","text":"First introduced Donald Knuth (Knuth 1984), aim Literate Programming able combine computer code text prose one document. allows analyst ability run code, view output code, view code provide prose description one document. way, literate programming document allows presenting analysis way performs computing steps desired presents easily readable format. Literate programming now key component creating distributing reproducible research (Gandrud 2015).","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"r-markdown","dir":"Articles","previous_headings":"","what":"R Markdown","title":"1. Literate programming I","text":"R Markdown specific implementation literate programming paradigm. Figure 1 see example R Markdown action. left see R Markdown source document right output (HTML) generated source document. Figure 1: R Markdown source output example. R Markdown documents generate various types output: web documents (.html), PDFs, Word documents, many types output formats. interleaving code prose one attractive aspects literate programming R Markdown, also possible create documents code . versatile technology come appreciate. like see R Markdown action please check Gallery R Markdown website. Note: Throughout Recipes can view source R Markdown document created recipe reading following link top page.  R Markdown source document plain-text file extension .Rmd can opened plain text reader. using RStudio IDE1 (henceforth RStudio) create, open, edit, generate output .Rmd files plain-text reader, TextEdit (MacOS) Notepad (PC) can open files. mind, let’s now move anatomy R Markdown document.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"structure","dir":"Articles","previous_headings":"R Markdown","what":"Structure","title":"1. Literate programming I","text":"basic level R Markdown document contains two components: (1) front-matter section (2) prose section. third component, code chunk, can interleaved within prose section add code document. Let’s look turn.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"front-matter","dir":"Articles","previous_headings":"R Markdown > Structure","what":"Front matter","title":"1. Literate programming I","text":"front matter R Markdown document appears, well, front document (top, rather). Referring back Figure 1 see front matter top. creating R Markdown document RStudio default attributes title, author, date, output. front matter bounded three dashes ---. values first three attributes pretty straightforward can edited needed. value output attribute can also edited tell .Rmd file generate output types. Can guess value might use generate PDF document? Yep, ’s just pdf_document. work R Markdown learn use RStudio interface change attributes add others!","code":"--- title: \"Introduction to R Markdown\" author: \"Jerid Francom\" date: \"8/23/2021\" output: html_document ---"},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"prose","dir":"Articles","previous_headings":"R Markdown > Structure","what":"Prose","title":"1. Literate programming I","text":"front matter contained within code chunk open prose. prose section(s) added functionality Markdown aware. mean, say? Well, ‘Markdown’ R Markdown refers fact can use various plain-text formatting conventions produce formatted text output document. quote Wikipedia Markdown lightweight markup language creating formatted text using plain-text editor. John Gruber Aaron Swartz created Markdown 2004 markup language appealing human readers source code form. Markdown widely used blogging, instant messaging, online forums, collaborative software, documentation pages, readme files. enables us add simple text conventions signal output formatted. Say want make text bold. just add ** around text want appear bold. can also : italics *italics* links [links](http://wfu.edu) strikethrough ~~strikethrough~~ etc. Follow link find information basic Markdown syntax.","code":"**bold text**"},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"code-chunks","dir":"Articles","previous_headings":"R Markdown > Structure","what":"Code chunks","title":"1. Literate programming I","text":"Code chunks R magic happens. , referring Figure 1 see following code chunk. code chunk bound three backticks ```. first backticks curly brackets {} allow us tell R Markdown programming language use evaluate (.e. run) code chunk. cases R, hence opening curly bracket `{r}`. good practice name code chunk(s). case `{r cars}`. line closing curly brackets code entered. example, R function (command) summary() used dataset cars produce summary dataset. ’s code chunk produces. mentioned selecting coding language naming code chunk, code chunks various options can used determine code chunk used. common code chunk options : hiding code hiding output etc.","code":"```{r cars} summary(cars) ``` summary(cars) ##      speed           dist     ##  Min.   : 4.0   Min.   :  2   ##  1st Qu.:12.0   1st Qu.: 26   ##  Median :15.0   Median : 36   ##  Mean   :15.4   Mean   : 43   ##  3rd Qu.:19.0   3rd Qu.: 56   ##  Max.   :25.0   Max.   :120 ```{r cars, echo=FALSE} summary(cars) ``` ##      speed           dist     ##  Min.   : 4.0   Min.   :  2   ##  1st Qu.:12.0   1st Qu.: 26   ##  Median :15.0   Median : 36   ##  Mean   :15.4   Mean   : 43   ##  3rd Qu.:19.0   3rd Qu.: 56   ##  Max.   :25.0   Max.   :120 ```{r cars, include=FALSE} summary(cars) ```"},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"create-and-render","dir":"Articles","previous_headings":"R Markdown","what":"Create and render","title":"1. Literate programming I","text":"easiest efficient way create R Markdown file use RStudio point--click’ interface. Just use tool bar create new file select “R Markdown”, seen Figure 2. Figure 2: Create R Markdown document RStudio toolbar. provide dialogue box asking add title author document also allows select type document output. Figure 3: RStudio dialogue box creating R Markdown document. clicking ‘OK’ get R Markdown document default/ boilerplate prose code chunks. can deleted can start document. now, let’s leave things see generate output report document. generate output report ‘render’ document, even ‘knit’ document. use render() function knitr package. Leaving functions packages aside can knit R Markdown document RStudio toolbar clicking ‘Knit’, seen . Figure 4: Knit R Markdown document RStudio toolbar. render, asked save file give name. done .Rmd file render format specified open ‘Viewer’ pane.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_1.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"1. Literate programming I","text":"concludes introduction literate programming using R Markdown. covered basics much explore. introduce R Markdown functionality next Recipe look ahead explore features recommend reading R Markdown (Get Started series). deeper dive, reference visit R Markdown: Definitive Guide (Xie, Allaire, Grolemund 2018)","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"10. Predictive models: prep, train, test, and evaluate","text":"Recipe build predictive model work spam filter. data work SMS (Short Message Service) text messages SMS Spam Collection (Almeida G’omez Hildago 2011). process steps include preparing dataset, splitting training testing sets, building training model, testing model, evaluating results. use Quanteda package (Benoit et al. 2022) package facilitates preparing, modeling, exploring text models. Let’s load packages use Recipe.","code":"library(tidyverse)           # data manipulation library(quanteda)            # tokenization and document-frequency matrices library(quanteda.textstats)  # descriptive text statistics library(quanteda.textmodels) # naive bayes classifier"},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"orientation","dir":"Articles","previous_headings":"Coding strategies","what":"Orientation","title":"10. Predictive models: prep, train, test, and evaluate","text":"Let’s first read become familiar SMS dataset. can access dataset tadr package. sms_df object data frame 5,574 observations two columns. observations correspond individual text messages. sms_type reflects type SMS message; either legitimate (‘ham’) spam message contains message text. recipe suffixing object names reflect object type. work dataset four main formats: data frame (_df), corpus _corpus, tokens _tokens, document-frequency matrix (_dfm). Let’s see proportion spam ham messages dataset. use tabyl() function janitor package get counts proportions. now know majority messages ‘ham’, around 87% exact.","code":"sms_df <- tadr::sms  # load dataset from tadr package glimpse(sms_df)  # preview dataset structure ## Rows: 5,574 ## Columns: 2 ## $ sms_type <chr> \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"spam\", \"ham\", \"ham\", \"sp… ## $ message  <chr> \"Go until jurong point, crazy.. Available only in bugis n gre… sms_df %>%     janitor::tabyl(sms_type)"},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"preparation","dir":"Articles","previous_headings":"Coding strategies","what":"Preparation","title":"10. Predictive models: prep, train, test, and evaluate","text":"now create Quanteda corpus object sms_df data frame. corpus object complex R object allow us manipulate data maintain metadata accessible format. create corpus object need call corpus() function set text_field argument identify column text . summary() function provides overview dataset including calculated text statistics (‘Types’, ‘Tokens’, ‘Sentences’) well metadata. corpus objects metadata known document variables, ‘docvars’. can access document variables directly docvars() function. can add metadata corpus object simply making reference new column assigning values . data going want unique document id 5,574 text messages. want create vector numbers 1 5,574. One way using 1:5574 syntax number hardcoded . Another flexible way create document id vector exactly fit number observations using ndoc() function corpus object . ndoc() return number documents corpus object. corpus objects observations known documents us document text message. can now look document variables see doc_id column now appears. note, time like convert corpus object back data frame can use tidy() function tidytext package.","code":"sms_corpus <- # quanteda corpus object   corpus(sms_df, # data frame          text_field = \"message\") # text field  sms_corpus %>%    summary(n = 5) # preview corpus object sms_corpus %>% # corpus object   docvars() %>%  # get corpus metadata attributes   slice_head(n = 5) # first observations sms_corpus$doc_id <- # create a new column `doc_id`   1:ndoc(sms_corpus) # add numeric id to each text message sms_corpus %>% # corpus object   docvars() %>%  # get corpus metadata attributes   slice_head(n = 5) # first observations sms_corpus %>% # corpus object   tidytext::tidy() %>% # convert back to a data frame   slice_head(n = 5) # first observations"},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"feature-engineering","dir":"Articles","previous_headings":"Coding strategies","what":"Feature engineering","title":"10. Predictive models: prep, train, test, and evaluate","text":"next step towards preparing dataset use predictive model decide features used help predictive model learn distinguish spam ham text messages. create language features tokenize text smaller linguistic units. can choose characters, words, sentences, ngram sequences either characters words. many predictive models, default feature select starting point words. However, reason believe linguistic unit(s) make practical sense nothing limiting selecting another unit. later see model perform well can always come back tweak tokenization process try linguistic units. Let’s move forward tokenizing messages words. also remove punctuation numbers lowercase messages. tokens() function allows us lowercasing. result assigned sms_tokens ‘tokens’ object. Looking first messages can see tokenized words. head() function used instead slice_head() function typically used. note head() works much like slice_head() main distinction slice_head() works data frame objects head() work type R object. case sms_tokens ‘tokens’ object complex type list object. nice tokens object metadata corpus object sms_corpus retained. can preview metadata docvars(). good know document variables can used group (tokens_group()), subset (tokens_subset()), sample (tokens_sample()) sms_tokens object much like functions tidyverse package working data frames. number functions can used manipulate tokens inside tokens object (tokens_ngrams(), tokens_toupper(), etc.). purposes use one functions named tokens_tolower() lowercase tokens. Now tokens ready use features prediction model. next step create Document-Frequency Matrix (DFM). structure unique token column unique document row. values (raw) counts tokens documents. create DFM use dfm() function. preview shows key information sms_dfm object. preview contains 5 documents, 9,313 features. features number unique tokens matrix. document number times unique token appeared message value. Since subset 9,313 possible feature tokens appear given message means may values 0. relative amount zeros non-zeros called sparsity. preview also see matrix 99.84% sparse. uncommon cases features number 10s thousands (also uncommon) matrix become quite large incur processing memory costs may need taken account. matrix unwieldly ’s current size won’t ‘trim’ , good know quanteda provides dfm_trim() function can used trim features either certain frequency count threshold (usually minimum frequency min_termfreq =) sparse (column-wise) certain percentage (sparsity =). Let’s explore features DFM. can use topfeatures() function retrieve frequent features sms_dfm object. can also use textstat_frequency() function quanteda.textstats package get frequency measures well group statistics document variable. case let’s look frequency statistics spam ham SMS types. can see grouped frequency statistics show similarities differences. one hand can see frequency counts document frequency much larger ham. makes sense since around 87% messages ham dataset. hand, look particular features see ‘’ common feature ham ‘’ spam. overlap well ‘’ ‘’ appear ham spam. overlap features less distinctive classes (ham spam) can make difficult prediction model make accurate predictions. moment continue move forward see model despite similarities, important know can apply techniques reduce similarity. One way return tokenization process remove common words using stopword list. approach common relies pre-defined lists considered common. Another approach weigh distribution given dataset number documents feature appears influences feature value. weighting called Term Frequency-Inverse Document Frequency (TF-IDF). can use dfm_tfidf() function transform sms_dfm matrix view weighting effects apparent overlap top features ham spam. can see now TF-IDF scores reduced apparent overlap. Let’s keep mind evaluate model performance. now continue raw counts value scores. last step need prepare dataset predictive modeling split dataset training testing sets. training set contain around 75% dataset 25% reserve testing. datasets similar relative proportions classes want predict (.e. ham spam). Let’s create numeric vector use randomly sample 75% observations sms_dfm use training. First set random number seed set.seed() make example reproducible. Next calculate number documents sms_dfm ndoc() function. multiply number .75 get sample size want. can use sample() function get train_ids. Note set replace = FALSE ids repeated sample (75% unique). training ids train_ids can now subset sms_dfm training set test set. Let’s verify proportions ham spam similar sms_dfm_train sms_dfm_test sets. , use tabyl() function. splits look comparable good proceed training prediction model.","code":"sms_tokens <-    tokens(x = sms_corpus, # corpus object          what = \"word\", # tokenize by words          remove_punct = TRUE, # remove punctuation          remove_numbers = FALSE) # remove numbers sms_tokens %>%     head(n = 5)  # preview first 5 tokenized messages ## Tokens consisting of 5 documents and 2 docvars. ## text1 : ##  [1] \"Go\"        \"until\"     \"jurong\"    \"point\"     \"crazy\"     \"Available\" ##  [7] \"only\"      \"in\"        \"bugis\"     \"n\"         \"great\"     \"world\"     ## [ ... and 8 more ] ##  ## text2 : ## [1] \"Ok\"     \"lar\"    \"Joking\" \"wif\"    \"u\"      \"oni\"    ##  ## text3 : ##  [1] \"Free\"  \"entry\" \"in\"    \"2\"     \"a\"     \"wkly\"  \"comp\"  \"to\"    \"win\"   ## [10] \"FA\"    \"Cup\"   \"final\" ## [ ... and 20 more ] ##  ## text4 : ##  [1] \"U\"       \"dun\"     \"say\"     \"so\"      \"early\"   \"hor\"     \"U\"       ##  [8] \"c\"       \"already\" \"then\"    \"say\"     ##  ## text5 : ##  [1] \"Nah\"    \"I\"      \"don't\"  \"think\"  \"he\"     \"goes\"   \"to\"     \"usf\"    ##  [9] \"he\"     \"lives\"  \"around\" \"here\"   ## [ ... and 1 more ] sms_tokens %>%    docvars() %>%  # get corpus metadata attributes   slice_head(n = 5) # first observations sms_tokens <-    sms_tokens %>% #    tokens_tolower() # lowercase all characters  sms_tokens %>%    head(n = 5) # preview first 5 tokenized messages ## Tokens consisting of 5 documents and 2 docvars. ## text1 : ##  [1] \"go\"        \"until\"     \"jurong\"    \"point\"     \"crazy\"     \"available\" ##  [7] \"only\"      \"in\"        \"bugis\"     \"n\"         \"great\"     \"world\"     ## [ ... and 8 more ] ##  ## text2 : ## [1] \"ok\"     \"lar\"    \"joking\" \"wif\"    \"u\"      \"oni\"    ##  ## text3 : ##  [1] \"free\"  \"entry\" \"in\"    \"2\"     \"a\"     \"wkly\"  \"comp\"  \"to\"    \"win\"   ## [10] \"fa\"    \"cup\"   \"final\" ## [ ... and 20 more ] ##  ## text4 : ##  [1] \"u\"       \"dun\"     \"say\"     \"so\"      \"early\"   \"hor\"     \"u\"       ##  [8] \"c\"       \"already\" \"then\"    \"say\"     ##  ## text5 : ##  [1] \"nah\"    \"i\"      \"don't\"  \"think\"  \"he\"     \"goes\"   \"to\"     \"usf\"    ##  [9] \"he\"     \"lives\"  \"around\" \"here\"   ## [ ... and 1 more ] sms_dfm <- dfm(sms_tokens)  # create a document-frequency matrix  sms_dfm %>%     head(n = 5)  # preview first 5 documents in the dfm ## Document-feature matrix of: 5 documents, 9,313 features (99.84% sparse) and 2 docvars. ##        features ## docs    go until jurong point crazy available only in bugis n ##   text1  1     1      1     1     1         1    1  1     1 1 ##   text2  0     0      0     0     0         0    0  0     0 0 ##   text3  0     0      0     0     0         0    0  1     0 0 ##   text4  0     0      0     0     0         0    0  0     0 0 ##   text5  0     0      0     0     0         0    0  0     0 0 ## [ reached max_nfeat ... 9,303 more features ] sms_dfm %>%     topfeatures(n = 10) ##    i   to  you    a  the    u  and   is   in   me  ## 2298 2252 2145 1446 1335 1168  979  895  891  800 sms_dfm %>%    textstat_frequency(n = 5, # get top 5 features                      groups = sms_type) # group by sms_type sms_dfm %>%    dfm_tfidf() %>% # calculate term frequency-inverse document frequency weighted scores   textstat_frequency(n = 5, # get top 5 features                      groups = sms_type, # group by sms_type                      force = TRUE) # force calculation despite weights set.seed(300) # make reproducible  num_docs <-    sms_dfm %>% # dfm object   ndoc() # get number of documents  train_size <-    (num_docs * .75) %>% # get size of sample   round() # round to nearest whole number  train_ids <- sample(x = 1:num_docs, # population                    size = train_size, # size of sample                    replace = FALSE) # without replacement  train_ids %>% head(n = 10) ##  [1] 2638  874 3650 3740  789  553 1705 4368 4557 2828 sms_dfm_train <-    sms_dfm %>% # dfm object   dfm_subset(doc_id %in% train_ids) # subset matching doc_id and train_ids  sms_dfm_test <-    sms_dfm %>% # dfm object   dfm_subset(!doc_id %in% train_ids) # subset non-matching doc_id and train_ids sms_dfm %>% # dfm object   docvars() %>% # pull the document variables   janitor::tabyl(sms_type) # check ham/spam proportions sms_dfm_train %>% # dfm object   docvars() %>% # pull the document variables   janitor::tabyl(sms_type) # check ham/spam proportions sms_dfm_test %>% # dfm object   docvars() %>% # pull the document variables   janitor::tabyl(sms_type) # check ham/spam proportions"},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"model-training","dir":"Articles","previous_headings":"Coding strategies","what":"Model training","title":"10. Predictive models: prep, train, test, and evaluate","text":"using Naive Bayes Classifier algorithm implemented quanteda.textmodels package textmodel_nb(). Naive Bayes common starting algorithm text classification. train model need pass sms_dfm_train document-feature matrix contains values feature tokens x class labels (‘ham’ ‘spam’) document using document variables sms_dfm_train object. assign model nb1 use summary() see overview training results. nb1 model summary see ‘Call’, ‘Class Priors’ preview ‘Estimated Feature Scores’. class priors set 50/50 ham spam means model assume one class prevalent . leave way don’t want bias model towards one class –even though input data biased. posterior probabilities features can seen estimated feature scores. given word can see model weighs individual features towards ham spam. predictions based sum probabilities features text message testing dataset. can see, example, probability ‘u’ higher ham ‘free’ leans towards ‘spam’. feature like ‘’, however, much discriminatory power split quite equally. can explore feature probabilities using coef() function nb1 model. can also see prediction scores document. prediction scores document, can transform can subsequently join actual class labels training dataset. ’s lot going code , goal get model’s prediction document training dataset include model’s probability score. Now nb1_predictions can bind column nb1$y model contains actual (original) class labels predictions. Now can cross-tabulate actual prediction table() send results confusionMatrix() function caret package provide summary model’s performance training dataset. trained model high accuracy score data trained . ’s perfect, practice algorithm . question now well trained model perform new dataset (sms_dfm_test). Let’s now move test evaluate model’s perform sms_dfm_train dataset.","code":"nb1 <-    textmodel_nb(x = sms_dfm_train, # document-feature matrix                y = sms_dfm_train$sms_type) # class labels  summary(nb1) # model summary ##  ## Call: ## textmodel_nb.dfm(x = sms_dfm_train, y = sms_dfm_train$sms_type) ##  ## Class Priors: ## (showing first 2 elements) ##  ham spam  ##  0.5  0.5  ##  ## Estimated Feature Scores: ##           go    until   jurong    point   crazy available    only     in ## ham  0.00323 0.000308 3.24e-05 1.62e-04 0.00013  0.000195 0.00159 0.0100 ## spam 0.00104 0.000260 4.33e-05 4.33e-05 0.00026  0.000173 0.00303 0.0023 ##         bugis       n    great    world       la        e   buffet     cine ## ham  1.14e-04 0.00180 0.001282 4.38e-04 9.73e-05 0.000973 3.24e-05 9.73e-05 ## spam 4.33e-05 0.00039 0.000476 4.33e-05 4.33e-05 0.000217 4.33e-05 4.33e-05 ##        there      got    amore      wat       ok      lar   joking      wif ## ham  0.00219 0.002920 3.24e-05 1.22e-03 0.003585 4.87e-04 6.49e-05 3.57e-04 ## spam 0.00052 0.000217 4.33e-05 8.66e-05 0.000173 4.33e-05 4.33e-05 4.33e-05 ##            u      oni     free    entry       2      a ## ham  0.01293 6.49e-05 0.000811 1.62e-05 0.00396 0.0133 ## spam 0.00533 4.33e-05 0.007233 8.66e-04 0.00585 0.0118 coef(nb1) %>% # get the feature coefficient scores   head # preview the feature coefficient scores ##                ham     spam ## go        3.23e-03 1.04e-03 ## until     3.08e-04 2.60e-04 ## jurong    3.24e-05 4.33e-05 ## point     1.62e-04 4.33e-05 ## crazy     1.30e-04 2.60e-04 ## available 1.95e-04 1.73e-04 predict(nb1, type = \"prob\") %>% # get the predicted document scores   head # preview predicted probability scores ##            ham     spam ## text1 1.00e+00 2.25e-08 ## text2 1.00e+00 9.58e-05 ## text3 3.31e-27 1.00e+00 ## text4 1.00e+00 1.23e-08 ## text5 1.00e+00 1.66e-11 ## text6 5.94e-04 9.99e-01 nb1_predictions <-    predict(nb1, type = \"prob\") %>% # get the predicted document scores   as.data.frame() %>% # convert to data frame   mutate(document = rownames(.)) %>% # add the document names to the data frame   as_tibble() %>% # convert to tibble   pivot_longer(cols = c(\"ham\", \"spam\"), # convert from wide to long format                names_to = \"prediction\", # new column for ham/spam predictions                values_to = \"probability\") %>% # probablity scores for each   group_by(document) %>% # group parameter by document   slice_max(probability, n = 1) %>% # keep the document row with highest probablity   slice_head(n = 1) %>% # for predictions that were 50/50    ungroup() %>% # remove grouping parameter   mutate(doc_id = str_remove(document, \"text\") %>% as.numeric) %>% # clean up document column so it matches doc_id in   arrange(doc_id) # order by doc_id  nb1_predictions %>%    slice_head(n = 10) # preview nb1_predictions_actual <-    cbind(actual = nb1$y, nb1_predictions) %>% # column-bind actual classes   select(doc_id, document, actual, prediction, probability) # organize variables  nb1_predictions_actual %>%    slice_head(n = 5) # preview tab_class <-    table(nb1_predictions_actual$actual, # actual class labels         nb1_predictions_actual$prediction) # predicted class labels  caret::confusionMatrix(tab_class, mode = \"prec_recall\") # model performance statistics ## Confusion Matrix and Statistics ##  ##        ##         ham spam ##   ham  3582   44 ##   spam   18  536 ##                                          ##                Accuracy : 0.985          ##                  95% CI : (0.981, 0.989) ##     No Information Rate : 0.861          ##     P-Value [Acc > NIR] : <2e-16         ##                                          ##                   Kappa : 0.937          ##                                          ##  Mcnemar's Test P-Value : 0.0015         ##                                          ##               Precision : 0.988          ##                  Recall : 0.995          ##                      F1 : 0.991          ##              Prevalence : 0.861          ##          Detection Rate : 0.857          ##    Detection Prevalence : 0.867          ##       Balanced Accuracy : 0.960          ##                                          ##        'Positive' Class : ham            ##"},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"model-testing","dir":"Articles","previous_headings":"Coding strategies","what":"Model testing","title":"10. Predictive models: prep, train, test, and evaluate","text":"test model first ensure features training test datasets matched dfm_matched(). use function predict() use nb1 model predict classes sms_dfm_train dataset without class labels.","code":"dfm_matched <-    dfm_match(sms_dfm_test, # test dfm             features = featnames(nb1$x)) # (left) join with trained model features  predicted_class <-    predict(nb1, # trained model           newdata = dfm_matched) # classify test dataset"},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"evaluation","dir":"Articles","previous_headings":"Coding strategies","what":"Evaluation","title":"10. Predictive models: prep, train, test, and evaluate","text":"Let’s now evaluate well model performs testing dataset. see trained nb1 model quite well 96.7% accuracy. dig cross-tabulation see errors tend cases model predicts messages spam fact ham. Let’s describe detail key statistics come calculated. summary meanings statistics: Accuracy: measure overall correct predictions Percentage predicted ‘ham’ messages correct Percentage actual ‘ham’ messages correct F1-score: summarizes balance precision recall calculate statistic hand helpful able read confusion matrix seen Figure 1. Figure 1: Confusion matrix mind can use tab_class confusion matrix extract TP, TN, FP, FN create calculations.","code":"actual_class <- dfm_matched$sms_type  # get actual class labels  tab_class <- table(actual_class, predicted_class)  # cross-tabulate actual and predicted class labels  caret::confusionMatrix(tab_class, mode = \"prec_recall\")  # model performance statistics ## Confusion Matrix and Statistics ##  ##             predicted_class ## actual_class  ham spam ##         ham  1161   40 ##         spam    6  187 ##                                          ##                Accuracy : 0.967          ##                  95% CI : (0.956, 0.976) ##     No Information Rate : 0.837          ##     P-Value [Acc > NIR] : < 2e-16        ##                                          ##                   Kappa : 0.871          ##                                          ##  Mcnemar's Test P-Value : 1.14e-06       ##                                          ##               Precision : 0.967          ##                  Recall : 0.995          ##                      F1 : 0.981          ##              Prevalence : 0.837          ##          Detection Rate : 0.833          ##    Detection Prevalence : 0.862          ##       Balanced Accuracy : 0.909          ##                                          ##        'Positive' Class : ham            ## tab_class  # view confusion matrix ##             predicted_class ## actual_class  ham spam ##         ham  1161   40 ##         spam    6  187 N <- sum(tab_class)  # sum of all predictions  TP <- tab_class[1, 1]  # positive, predicted positive TN <- tab_class[2, 2]  # negative, predicted negative  FP <- tab_class[1, 2]  # negative, predicted positive FN <- tab_class[2, 1]  # positive, predicted negative  # Summary statistics  accuracy <- (TP + TN)/N  # higher correct predictions (TP and TN) increase accuracy precision <- TP/(TP + FP)  # lower FP increases precision recall <- TP/(TP + FN)  # lower FN increases recall  f1_score <- 2 * ((precision * recall)/(precision + recall))"},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"production","dir":"Articles","previous_headings":"Coding strategies","what":"Production","title":"10. Predictive models: prep, train, test, and evaluate","text":"Now, satisfied model wanted put use filter real SMS text messages, create function just . just need include step tokenize new messages way create model create dfm tokens features. match features model apply model new message report class prediction result. can see can now create text messages filtered algorithm developed .","code":"predict_sms_type <- function(sms_message, nb_model) {   # Function   # Takes a character vector of sms messages and provides   # a prediction as to whether the message is spam or ham   # given the given trained NB model    sms_tokens <-    tokens(x = sms_message, # character vector          what = \"word\", # tokenize by words          remove_punct = TRUE, # remove punctuation          remove_numbers = FALSE) %>% # remove numbers   tokens_tolower() # lowercase all characters  sms_dfm <-    dfm(sms_tokens) # create a document-frequency matrix      # Match features from the Naive Bayes model   dfm_matched <-      dfm_match(sms_dfm, features = featnames(nb_model$x))      # Predict class for the given review   predicted_class <- predict(nb_model, newdata = dfm_matched)      as.character(predicted_class) }   predict_sms_type(sms_message = \"Hey how's it going?\", nb_model = nb1) ## [1] \"ham\" predict_sms_type(sms_message = \"Call now to order this amazing product!!\", nb_model = nb1) ## [1] \"spam\""},{"path":"https://lin380.github.io/tadr/articles/recipe_10.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"10. Predictive models: prep, train, test, and evaluate","text":"Recipe, demonstrated steps involved creating text classification model. evaluated training model applied test data. evaluation training testing predictions show highly accurate. Since results promising demonstrated can turn trained model actual spam filter. ’ve included code summary necessary steps implement prediction model.","code":"# Get SMS dataset --- sms_df <- tadr::sms # load dataset from tadr package  # Create corpus object --- sms_corpus <- # quanteda corpus object   corpus(sms_df, # data frame          text_field = \"message\") # text field  sms_corpus$doc_id <- # create a new column `doc_id`   1:ndoc(sms_corpus) # add numeric id to each text message  # Create tokens object sms_tokens <-    tokens(x = sms_corpus, # corpus object          what = \"word\", # tokenize by words          remove_punct = TRUE, # remove punctuation          remove_numbers = FALSE) %>% # remove numbers   tokens_tolower() # lowercase all characters  # Create testing/ training splits set.seed(300) # make reproducible  num_docs <-    sms_dfm %>% # dfm object   ndoc() # get number of documents  train_size <-    (num_docs * .75) %>% # get size of sample   round() # round to nearest whole number  train_ids <- sample(x = 1:num_docs, # population                    size = train_size, # size of sample                    replace = FALSE) # without replacement sms_dfm_train <-    sms_dfm %>% # dfm object   dfm_subset(doc_id %in% train_ids) # subset matching doc_id and train_ids  sms_dfm_test <-    sms_dfm %>% # dfm object   dfm_subset(!doc_id %in% train_ids) # subset non-matching doc_id and train_ids  # Train the NB model nb1 <-    textmodel_nb(x = sms_dfm_train, # document-feature matrix                y = sms_dfm_train$sms_type) # class labels  # Test the NB model dfm_matched <-    dfm_match(sms_dfm_test, # test dfm             features = featnames(nb1$x)) # (left) join with trained model features  predicted_class <-    predict(nb1, # trained model           newdata = dfm_matched) # classify test dataset  # Evaluate model performance actual_class <-    dfm_matched$sms_type # get actual class labels  tab_class <-    table(actual_class, predicted_class) # cross-tabulate actual and predicted class labels  caret::confusionMatrix(tab_class, mode = \"prec_recall\") # model performance statistics"},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"Recipe aim explore Rate Professor sample dataset (2020). investigate language associated higher lower course ratings using various methods. lean heavily Quanteda package (Benoit et al. 2022) preparing, exploring, visualizing student ratings. Let’s load packages use Recipe.","code":"library(tidyverse)           # data manipulation library(patchwork)           # organize plots library(janitor)             # cross tabulations library(tidytext)            # text operations library(quanteda)            # tokenization and document-frequency matrices library(quanteda.textstats)  # descriptive text statistics library(quanteda.textmodels) # topic modeling library(quanteda.textplots)  # plotting quanteda objects"},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"orientation","dir":"Articles","previous_headings":"Coding strategies","what":"Orientation","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"Let’s get familiar Rate Professor dataset. rmp_df data frame contains 19,026 observations 6 variables. variable corresponds course rating. (Note dataset curated removing many variables explored Recipe observations ‘comments’ removed.) Look number ratings course_rating category see many ‘high’ ‘low’ ratings dataset. can see 19026 observations roughly 60% labeled ‘high’ rest ‘low’. look student_star variable provided students can see distribution. case use visualization easily see distribution given number levels.  see good proportion courses rated ‘5’ students followed ‘1’ scattering rating values (skewed towards positive values). seem drop 3.5 4 (Note curated dataset splitting ‘high’ ‘low’ 3.5!)","code":"rmp_df <- read_csv(file = \"recipe_11/data/derived/rate_my_professor_sample/rmp_curated.csv\")  # read data  glimpse(rmp_df)  # preview ## Rows: 19,026 ## Columns: 6 ## $ doc_id        <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… ## $ student_id    <dbl> 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, … ## $ student_star  <dbl> 5.0, 5.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.5, 5… ## $ course_rating <chr> \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", … ## $ online        <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, … ## $ comments      <chr> \"This class is hard, but its a two-in-one gen-ed knockou… rmp_df %>% # data frame   tabyl(course_rating) # cross-tabulate rmp_df %>% # data frame   tabyl(student_star) %>% # cross-tabulate   ggplot(aes(x = as.factor(student_star), y = (percent * 100))) +   geom_col() +   labs(x = \"Student star\", y = \"Percent\")"},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"preparation","dir":"Articles","previous_headings":"Coding strategies","what":"Preparation","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"Let’s now prepare data frame exploration. first create Quanteda corpus object using comments variable text field. apply summary() function corpus object (rmp_corpus) assign output new object rmp_corpus_summary. Note summary() function return 100 documents default, let’s set number documents return capture documents. hardcode number documents corpus object (n = 19026) can use ndoc() function find number automatically –let’s take second approach. Let’s take sample rmp_corpus_summary get idea summary statistics available. Now something think might important consider different rating distributions -person online courses. Let’s visualize difference. Figure 1: Relationship course rating modality. Figure 1 see raw counts B see proportion. can see majority courses online, -person. appears slightly low ratings online courses (seen B). Let’s test difference see fact effect modality dataset. create cross-tabulation course_rating online run Chi-squared test table. results suggest fact significant effect modality. Given finding fact relatively online courses dataset, let’s remove online courses dataset can focus -person courses. can use corpus_subset() function quanteda. Now let’s get summary statistics new subsetted corpus object. Let’s now see distribution course_rating changed result removing online courses. proportions similar. now -person classes make subsequent exploration bit homogeneous therefore straightforward interpret.","code":"rmp_corpus <-    rmp_df %>% # data frame   corpus(text_field = \"comments\") # create corpus object  rmp_corpus_summary <-    rmp_corpus %>% # corpus object   summary(n = ndoc(rmp_corpus)) # get summary information from the corpus object set.seed(123) # set seed for reproducible sampling  rmp_corpus_summary %>% # corpus summary   slice_sample(n = 10) # sample 10 observations/ documents p1 <- rmp_corpus_summary %>%     ggplot(aes(x = course_rating, fill = online)) + geom_bar(show.legend = FALSE) +     labs(x = \"Course rating\", y = \"Count\")  p2 <- rmp_corpus_summary %>%     ggplot(aes(x = course_rating, fill = online)) + geom_bar(position = \"fill\") +     labs(x = \"Course rating\", y = \"Proportion\", fill = \"Online\")  p1 + p2 + plot_annotation(tag_levels = \"A\") online_rating_tab <-    xtabs(~course_rating + online, # relationship         data = rmp_corpus_summary) # dataset  online_rating_tab # cross-tabulation ##              online ## course_rating FALSE  TRUE ##          high 11130   199 ##          low   7494   203 c1 <- chisq.test(online_rating_tab) # chi-squared test  c1$p.value < .05 # statistical confirmation ## [1] TRUE rmp_corpus <- # new corpus object   rmp_corpus %>% # original corpus object   corpus_subset(online == FALSE) # remove online courses rmp_corpus_summary <-    rmp_corpus %>% # corpus object   summary(n = ndoc(rmp_corpus)) # get summary information from the corpus object rmp_corpus_summary %>% # data frame   tabyl(course_rating) # cross-tabulate"},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"exploration","dir":"Articles","previous_headings":"Coding strategies","what":"Exploration","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"Now let’s start explore language use dataset.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"frequency-analysis","dir":"Articles","previous_headings":"Coding strategies > Exploration","what":"Frequency analysis","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"first step, let’s consider word frequency distributions. can use rmp_corpus_summary object contains Types Tokens counts courses. Let’s look descriptive visualizations distribution tokens. Figure 2: Token distribution Rate Professor dataset. Figure 2 B see distribution histogram density plot. appears drop number tokens around 75. C see boxplot suggests majority course ratings token counts around 25 70 tokens larger tail higher end. One question may interested exploring whether students write less depending overall rating course. Let’s visualize potential relationship creating boxplot compares course_rating Tokens. Figure 3: Distribution tokens course rating. boxplot Figure 3 suggests fact students rate course lower tend write . can feel confident making assessment notches boxplot overlap ‘high’ ‘low’ course ratings. finding may jibe intuition –student takes time rate course don’t like want say . may explanations, finding interest.","code":"p1 <-    rmp_corpus_summary %>% # corpus summary data frame   ggplot(aes(x = Tokens)) + # mappings   geom_histogram(binwidth = 5) + # histogram, 5-token bin groupings   labs(y = \"Count\") # labels  p2 <-    rmp_corpus_summary %>% # corpus summary data frame   ggplot(aes(x = Tokens)) + # mappings   geom_density() + # density plot   labs(y = \"Density\") # labels  p3 <-    rmp_corpus_summary %>% # corpus summary data frame   ggplot(aes(y = Tokens)) + # mappings   geom_boxplot(notch = TRUE) + # box plot   scale_y_continuous(breaks = seq(0, 135, 10)) + # add y-axis breaks   theme(axis.text.x = element_blank()) # remove x-axis breaks  (p1 / p2 | p3) + # positions   plot_annotation(title = \"Token distribution\",                    tag_levels = \"A\") # annotations rmp_corpus_summary %>% # corpus summary data frame   ggplot(aes(x = course_rating, y = Tokens)) + # mappings   geom_boxplot(notch = TRUE) + # box plot with notches   labs(x = \"Course rating category\") # labels"},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"keyness-analysis","dir":"Articles","previous_headings":"Coding strategies > Exploration","what":"Keyness analysis","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"Now let’s take look words compare words indicative either ‘high’ ‘low’ course ratings. called keyness analysis. First need create tokens object. tokenize corpus words removing punctuation numbers lowercasing tokens. convert tokens object document-frequency matrix (dfm) using raw counts (term weights). rmp_dfm can use textstat_keyness() function get indicative features. specify target sub-corpus (‘high’) use comparison. Since two levels (‘high’ ‘low’) reference sub-corpus ‘low’. can visualize contrasts textplot_keyness() function. retrieve top 25 contrastive terms. Nothing surprising . Courses rated high terms like ‘great’, ‘best’, ‘awesome’, etc. low-rated courses contain terms like ‘’, ‘worst, ’avoid’, etc. Now used words terms analysis may interesting see two-word sequences, bigrams, may contrastive show something interesting / unexpected. Let’s create new document-frequency matrix containing bigrams. can just pass rmp_tokens object tokens_ngrams() converting result document-frequency matrix. can run steps word-based term keyness analysis. , see terms seem expected. However, one bigram catches interest. bigram guy_but shows gendered term. makes wonder similarities differences ratings / language associated gender instructor course. Let’s explore gendered language ratings first. start creating character vector contains set gendered terms, one males (male) later one females (female). pass rmp_tokens object tokens_keep() function. pattern = argument specifies terms want identify documents window = argument specifies many adjacent terms want keep around pattern. preview rmp_tokens_male object shows us documents match one terms male retained. Let’s thing now female-oriented gender terms. now convert gendered token objects document-frequency matrices. keep documents dfm gendered language pull document variables dfm. dfm’s document variable data frame add new column gender appropriate gendered set. complete can combine two data frames rows can see now data frame 15,118 observations (courses) 5 variables (including original metadata new gender column). Let’s now visualize relationship course_rating gender. Figure 4: Relationship course rating gender. Figure 4 B can clearly see appears difference course rating documents male- female-oriented terms. can perform Chi-squared test verify. rating (‘high’ versus ‘low’) given course seem differ given appearance gendered terms. second step explore gendered terms potential language associations. use keyness analysis, time use gendered terms. Looking documents male-gendered terms need create reference sub-corpus –documents male-gendered terms. Note include documents female-gendered terms documents gendered terms . join male-gendered target sub-corpus reference corpus add column type ‘target’ ‘reference’ document-frequency matrices can specify target perform keyness analysis. Now can generate keyness measures textstat_keyness() plot constrastive terms. Note ’ve removed gendered terms filter() also removed terms less three characters (get rid prepositions, pronouns, etc.). see male gendered documents series indicative features particularly telling: ‘funny’, ‘jokes’, ‘hilarious’, ‘cool’, ‘smart’, ‘nice’, ‘cares’. bulk point documents male-oriented gender terms also noting jovial personality instructor. ’s much can make ‘reference’ terms mixed-gendered bag female-gendered terms documents gendered terms. Let’s now run analysis, time female-gendered terms. indicative terms female-gendered documents see bit mixed bag. potentially two groups interest: positive traits ‘sweet’, ‘wonderful’, ‘nice’, ‘helpful’, etc. negative traits ‘rude’ ‘feminist’ (’m assuming term negative context).","code":"rmp_tokens <-    rmp_corpus %>% # corpus object   tokens(what = \"word\", # tokenize by words          remove_punct = TRUE, # remove punctuation          remove_numbers = TRUE) %>% # remove numbers   tokens_tolower() # lowercase the tokens rmp_dfm <-    rmp_tokens %>% # tokens object   dfm() # create document-frequency matrix rmp_dfm %>%    textstat_keyness(target = rmp_dfm$course_rating == \"high\") %>% # keyness measures   textplot_keyness(show_legend = FALSE, n = 25, labelsize = 3) + # plot most contrastive terms   labs(x = \"Chi-squared statistic\",       title = \"Term keyness\",         subtitle = \"Course ratings: high versus low\") # labels rmp_dfm_bigrams <-    rmp_tokens %>% # tokens object   tokens_ngrams(n = 2) %>% # create 2-term ngrams (bigrams)   dfm() # create document-frequency matrix rmp_dfm_bigrams %>%    textstat_keyness(target = rmp_dfm_bigrams$course_rating == \"high\") %>%    textplot_keyness(show_legend = FALSE, n = 25, labelsize = 3) + # plot most contrastive terms   labs(x = \"Chi-squared statistic\",       title = \"Term keyness (bigrams)\",         subtitle = \"Course ratings: high versus low\") # labels male <- c(\"he\", \"him\", \"his\", \"himself\", \"mr\", \"man\", \"guy\", \"hes\")  rmp_tokens_male <-    rmp_tokens %>% # tokens object   tokens_keep(pattern = male, # terms to identify               window = 10) # also keep adjacent 10 terms  rmp_tokens_male %>%    head() # preview documents that match one or more of our gendered terms in `male` ## Tokens consisting of 6 documents and 4 docvars. ## 1 : ## character(0) ##  ## 2 : ## character(0) ##  ## 3 : ## character(0) ##  ## 4 : ## character(0) ##  ## 5 : ##  [1] \"professor\" \"looney\"    \"has\"       \"great\"     \"knowledge\" \"in\"        ##  [7] \"astronomy\" \"while\"     \"he\"        \"can\"       \"explain\"   \"them\"      ## [ ... and 33 more ] ##  ## 6 : ##  [1] \"looney\"      \"is\"          \"a\"           \"super\"       \"funny\"       ##  [6] \"guy\"         \"and\"         \"this\"        \"class\"       \"was\"         ## [11] \"really\"      \"interesting\" ## [ ... and 25 more ] female <- c(\"she\", \"her\", \"herself\", \"ms\", \"mrs\", \"woman\", \"lady\", \"shes\")  rmp_tokens_female <-    rmp_tokens %>% # tokens object   tokens_keep(pattern = female, # terms to identify               window = 10) # also keep adjacent 10 terms rmp_dfm_male <- dfm(rmp_tokens_male) # dfm for male-gendered terms  rmp_male_docvars <-    rmp_dfm_male %>% # male-gendered dfm   dfm_subset(rowSums(rmp_dfm_male) > 0) %>% # keep documents with male-gendered terms   docvars() # pull the document variables  rmp_male_docvars$gender <- \"male\" # add a column `gender` and set to 'male'   rmp_dfm_female <- dfm(rmp_tokens_female) # dfm for female-gendered terms  rmp_female_docvars <-    rmp_dfm_female %>% # female-gendered dfm   dfm_subset(rowSums(rmp_dfm_female) > 0) %>% # keep documents with female-gendered terms   docvars() # pull the document variables  rmp_female_docvars$gender <- \"female\" # add a column `gender` and set to 'female'  rmp_docvars_gender <-    rbind(rmp_male_docvars, rmp_female_docvars) # combine by rows  glimpse(rmp_docvars_gender) # preview combined data frame ## Rows: 15,118 ## Columns: 5 ## $ student_id    <dbl> 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 8, 8, 8, 8, … ## $ student_star  <dbl> 5.0, 5.0, 5.0, 5.0, 4.5, 5.0, 4.0, 4.0, 5.0, 4.5, 5.0, 1… ## $ course_rating <chr> \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", … ## $ online        <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, … ## $ gender        <chr> \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", … p1 <-    rmp_docvars_gender %>%    ggplot(aes(x = course_rating, fill = gender)) + # mappings   geom_bar(show.legend = FALSE) + # bar plot (no legend)   labs(y = \"Count\", x = \"Course rating\")  p2 <-    rmp_docvars_gender %>%    ggplot(aes(x = course_rating, fill = gender)) + # mappings   geom_bar(position = \"fill\") + # bar plot   labs(y = \"Proportion\", x = \"Course rating\", fill = \"Gender\")  p1 + p2 + plot_annotation(tag_levels = 'A') rating_gender_tab <- xtabs(~course_rating + gender, data = rmp_docvars_gender)  # cross-tabulation  c1 <- chisq.test(rating_gender_tab)  # chi-squared test  c1$p.value < 0.05  # verify p-value ## [1] FALSE rmp_tokens_male_reference <-    rmp_tokens %>% # tokens object   tokens_remove(pattern = male, window = 10) %>% # remove male-gendered terms   dfm() # create document-frequency matrix rmp_dfm_male$type <- \"target\"  # set 'target' rmp_tokens_male_reference$type = \"reference\"  # set 'reference'  rmp_dfm_male_keyness <- rbind(rmp_dfm_male, rmp_tokens_male_reference)  # combine by rows keyness_target_male <-    rmp_dfm_male_keyness %>%    textstat_keyness(target = rmp_dfm_male_keyness$type == \"target\") # keyness measures  keyness_target_male %>%    filter(!feature %in% male) %>% # remove male-gendered terms   filter(nchar(feature) > 3) %>% # remove short terms (< 3 characters)   textplot_keyness() +  # plot the indicative features.   labs(x = \"Chi-squared statistic\",       title = \"Term keyness\",         subtitle = \"Gendered documents: male versus others\") # labels rmp_tokens_female_reference <-    rmp_tokens %>%    tokens_remove(pattern = female, window = 10) %>%  # remove female-gendered terms   dfm() # create document-frequency matrix  rmp_dfm_female$type <- \"target\" # set 'target' rmp_tokens_female_reference$type = \"reference\" # set 'reference'    rmp_dfm_female_keyness <-    rbind(rmp_dfm_female, rmp_tokens_female_reference) # combine by rows  keyness_target_female <-    rmp_dfm_female_keyness %>%    textstat_keyness(target = rmp_dfm_female_keyness$type == \"target\") # keyness measures  keyness_target_female %>%    filter(!feature %in% female) %>% # remove female-gendered terms   filter(nchar(feature) > 3) %>% # remove short terms (< 3 characters)   textplot_keyness() + # plot the indicative features.   labs(x = \"Chi-squared statistic\",       title = \"Term keyness\",         subtitle = \"Gendered documents: male versus others\") # labels"},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"sentiment-analysis","dir":"Articles","previous_headings":"Coding strategies > Exploration","what":"Sentiment analysis","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"findings two keyness analysis gendered terms makes wonder differences sentiment comments sentiments may may relate course rating. Let’s perform simple dictionary approach sentiment analysis dataset. First need get sentiment lexicon. ’ve chosen use NRC Word-Emotion Association Lexicon (Mohammad Turney 2013) can loaed textdata package (Hvitfeldt 2020). lexicon contains 13,875 words classified “positive”, “joy”, “trust”, “anticipation”, “surprise”, “negative”, “anger”, “fear”, “disgust”, “sadness”. sentiment lexicon quanteda dictionary format, use tokens_lookup() function label tokens object sentiment words appear tokens dictionary objects. rmp_tokens_male rmp_tokens_female tokens objects. convert dfm objects, generate frequency measures (sentiments features), create barplot contrasting course ratings. lot going code generate plots. can refer ggplot2 documentation information. Figure 5: Sentiment analysis male- female-gendered documents relationship course ratings. Looking plots see positive sentiment categories proportion course ratings quite similar male female. However, look negative sentiment categories see categories make larger proportion (around 5% ) compared negative categories males. suggests although category rating may students tend choose choose negatively charged words comments course course instructor female.","code":"sentiment_dictionary <-    textdata::lexicon_nrc() %>% # load the nrc lexicon   as.dictionary() # convert the data frame to a dictionary object  rmp_tokens_male_sentiment <-    rmp_tokens_male %>%    tokens_lookup(dictionary = sentiment_dictionary) # add sentiment labels  p1 <-    rmp_tokens_male_sentiment %>%    dfm() %>% # create dfm   textstat_frequency(groups = course_rating) %>% # generate frequency measures   mutate(feature_fct = factor(feature, levels = c(\"positive\", \"joy\", \"trust\", \"anticipation\", \"surprise\", \"negative\", \"anger\", \"fear\", \"disgust\", \"sadness\"))) %>% # reorder features from positive to negative   group_by(feature_fct) %>% # grouping parameter   mutate(prop = round(frequency/sum(frequency), 2)) %>% # create proportions scores    ggplot(aes(x = feature_fct, y = frequency, fill = group, label = paste0(prop))) + # mappings   geom_bar(stat = \"identity\", position = \"fill\") + # proportion bar plot   geom_label(position = \"fill\", vjust = 2) + # add proportions labels   labs(title = \"Male instructor word sentiment\", fill = \"Course rating\", y = \"Proportion\", x = \"\") # labels  rmp_tokens_female_sentiment <-    rmp_tokens_female %>%    tokens_lookup(dictionary = sentiment_dictionary) # add sentiment labels  p2 <-    rmp_tokens_female_sentiment %>%    dfm() %>% # create dfm   textstat_frequency(groups = course_rating) %>% # generate frequency   mutate(feature_fct = factor(feature, levels = c(\"positive\", \"joy\", \"trust\", \"anticipation\", \"surprise\", \"negative\", \"anger\", \"fear\", \"disgust\", \"sadness\"))) %>% # reorder features from positive to negative   group_by(feature_fct) %>% # grouping parameter   mutate(prop = round(frequency/sum(frequency), 2)) %>% # create proportion scores   ggplot(aes(x = feature_fct, y = frequency, fill = group, label = paste0(prop))) + # mappings   geom_bar(stat = \"identity\", position = \"fill\") + # proportion bar plot   geom_label(position = \"fill\", vjust = 2) + # add proportion labels   labs(title = \"Female instructor word sentiment\", fill = \"Course rating\", y = \"Proportion\", x = \"Sentiment\") # labels  p1 / p2 + plot_annotation(tag_levels = 'A')"},{"path":"https://lin380.github.io/tadr/articles/recipe_11.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"11. Exploratory methods: descriptive and unsupervised learning analysis methods","text":"Recipe, started Rate Professor dataset. steppe process getting oriented dataset, preparing dataset analysis, conducting three exploratory analysis types: frequency, keyness, sentiment. made interesting findings: 1) students tend use words rate course lower, 2) students appear rate courses male female instructors differently, 3) indication word types used describe male female instructors, 4) although course ratings seem associated language student comments, noted potential relationship stronger negatively charged words used describe female instructors’ courses. much done dataset, enough pique interest exploratory data analysis.","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_2.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"2. Literate programming II","text":"Recipe going explore additional functionality R Markdown. include number sections, add table contents, add -line citations document-final references list, cross-reference tables figures.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_2.html","id":"toolbar-options","dir":"Articles","previous_headings":"","what":"Toolbar options","title":"2. Literate programming II","text":"Let’s explore functionality RStudio provides quick access toolbar. RStudio toolbar right ‘Knit’ button, cog icon. Click dropdown menu icon select ‘Output Options…’, seen Figure 1. Figure 1: RStudio ‘Output Options…’. dialogue box open (Figure 2). dialogue box, provides option add document output type front matter (three principle types –HTML, PDF, Word) / modify output type options. Looking general options HTML, can add table contents, change theme colors (code syntax general), add numbers section headings (among options). Figure 2: RStudio toolbar document options. check “Include table contents” “Number section headings” click “OK”, see front matter modified RStudio –specifically, last four lines. Now ‘Knit’ R Markdown document look like . Figure 3: HTML output table contents numbered section headings. nothing magical RStudio done. changed front matter manually include functionality, see next section. toolbar, however, provide convenient access common options great way get started modifying R Markdown front matter.","code":"--- title: \"Literate Programming II\" author: \"Jerid Francom\" date: \"8/25/2021\" output:    html_document:      toc: yes     number_sections: yes ---"},{"path":"https://lin380.github.io/tadr/articles/recipe_2.html","id":"manual-options","dir":"Articles","previous_headings":"","what":"Manual options","title":"2. Literate programming II","text":"options provided RStudio toolbar commonly used many potential attributes can use modify R Markdown document’s output. good first resource exploring options functionality accessible RStudio toolbar R Markdown: Definitive Guide (Xie, Allaire, Grolemund 2021). section Output Formats provides descriptions attributes values many output types. Let’s look HTML document section Table contents output options. can see can manually add front matter seen specifying level header depth include (toc_depth:) want table contents positioned floating left document (toc_float:), well floating table contents behave. add options R Markdown document manually, need make sure tab-indent attribute appropriate attribute modifies. , add table contents html_document: attribute, toc: attribute tab-indented html_document:. value takes, want appear true yes (either work). Floating table contents also modifies html_document: attribute, also tab-indented fashion. , however, want modify table contents floats (say, table contents ‘collapsed’ default), attributes need tab-indented relative toc_float: attribute. configuration just described shown . Now ‘Knit’ R Markdown document new attribute/ value pairs look like . Figure 4: HTML output added floating table contents. important note: output formats available options, simple nature type output document. example, floating table contents makes sense HTML output, PDF Word documents. mindful output format using options available.","code":"--- title: \"Literate Programming II\" author: \"Jerid Francom\" date: \"8/25/2021\" output:    html_document:      toc: yes     number_sections: yes     toc_float:       collapsed: yes ---"},{"path":"https://lin380.github.io/tadr/articles/recipe_2.html","id":"advanced-options","dir":"Articles","previous_headings":"","what":"Advanced options","title":"2. Literate programming II","text":"many options explore. good resource explore advanced functionality R Markdown Cookbook (Xie, Dervieux, Riederer 2021). resource find information useful creating research reports: bibliographies citations cross-referencing within documents.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_2.html","id":"bibliographies-and-citations","dir":"Articles","previous_headings":"Advanced options","what":"Bibliographies and citations","title":"2. Literate programming II","text":"ability add citations references R Markdown document useful option documenting literature report. three things R Markdown document needs able include -line citations document-final references section. bibliography file .bib (BibTeX) format project’s main directory (folder) bibliography: attribute value specifying name .bib file front matter inline citation includes citation key reference .bib file part prose section Let’s take elements turn. First, .bib file just plain text file .bib extension. file, reference’s key information formatted according BibTeX format includes citation key. Figure 5: bibliography file (.bib) format. Next add bibliography: attribute name .bib file value. attribute indented. Finally, make reference citation key .bib file prose R Markdown document using -line citation. citation key always includes @citation-key format. include citation parentheses, add brackets around citation key [@citation-key]. example reference use citation key tottie2011, include prose like : Knitting document produces -line citation adds full reference end document. Notice ’ve added section header ‘References’ end R Markdown document references appear underneath. Figure 6: HTML output -line citation full reference. Now manage references, get BibTeX format, add .bib file RStudio Cloud use Zotero. following video describe process set use Zotero RStudio Cloud (length 22:02 minutes).","code":"--- title: \"Literate Programming II\" author: \"Jerid Francom\" date: \"8/25/2021\" output:    html_document:      toc: yes     number_sections: yes     toc_float:       collapsed: yes bibliography: references.bib --- ## R Markdown  This is an in-line citation [@tottie2011]."},{"path":"https://lin380.github.io/tadr/articles/recipe_2.html","id":"cross-referencing-within-documents","dir":"Articles","previous_headings":"Advanced options","what":"Cross-referencing within documents","title":"2. Literate programming II","text":"Another useful feature reports ability reference tables graphics R Markdown document, seen Figure 7. Figure 7: HTML output figure table cross-references. standard R Markdown document outputs (html_document, pdf_document word_document) provide functionality. able create cross-references, need use bookdown package. already installed, need first R Console. Note: LIN 380 workspace RStudio Cloud, bookdown already installed. , need edit front matter make use document outputs html_document2, pdf_document2 word_document2. output formats need make reference package directly, include bookdown::html_document2, seen . Next need find code chunk name make reference make sure caption associated figure table. example code , code chunk name cars-figure code chunk option fig.cap='' added caption quotes. table, , find code-chunk name (cars-table) make sure caption associated. Captions tables, however, part code chunk options, rather function creates table needs add caption. example , ’ve used kable() function knitr package (referenced directly knitr::kable()). One arguments kable() function caption =. Now ducks line. cross-reference appear -line, use following format: figures: \\@ref(fig:code-chunk-name) tables: \\@ref(tab:code-chunk-name) make reference example plot code chunk name cars-figure, -line reference \\@ref(fig:cars-figure). table cars-table, \\@ref(tab:cars-table). automatically generate figure table numbers, numbers, order appear R Markdown document. Just add ‘Figure’ ‘Table’ reference appear seen Figure 7 . mentioned earlier, bookdown also cross-reference enabled pdf_document2 word_document2. allows us create PDF Word documents cross-referencing well. Note: attributes associated floating table contents removed pdf_document2 output applicable PDF documents. case word_document2.","code":"# install the bookdown package install.packages(\"bookdown\") --- title: \"Literate Programming II\" author: \"Jerid Francom\" date: \"8/25/2021\" output:    bookdown::html_document2:      toc: yes     number_sections: yes     toc_float:       collapsed: yes bibliography: references.bib --- ```{r cars-figure, fig.cap='Example plot'} plot(cars) ``` ```{r cars-table} knitr::kable(head(cars), caption = \"Example table.\") ``` --- title: \"Literate Programming II\" author: \"Jerid Francom\" date: \"8/25/2021\" output:    bookdown::pdf_document2:      toc: yes     number_sections: yes bibliography: references.bib ---"},{"path":"https://lin380.github.io/tadr/articles/recipe_2.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"2. Literate programming II","text":"Recipe introduced variety features can add R Markdown documents help us generate informative reports. included number sections, add table contents, add -line citations document-final references list, cross-reference tables figures. next Recipe turn attention working R. continue use R Markdown, however, vehicle interleave R code prose description continue work extend knowledge gained first two Recipes Literate Programming R Markdown.","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"3. Reading, inspecting, and writing data","text":"Recipe now start turn focus using literate programming R markdown work R coding strategies. means make extensive use ‘code chunks’. actual R code entered, work done, output R code returns value, table, graphic, appear document output. focus reading, inspecting, writing data.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"3. Reading, inspecting, and writing data","text":"get work, let’s housekeeping set R Markdown document type work going . First, common include code chunk top prose section. use code chunk load necessary R packages functions use extensively document. can also used various settings affect code execution display document-wide. typical name chunk ‘setup’ add code-chunk option-value pair message=FALSE suppress message information typically returned loading packages. ’m going load tidyverse package use extensively Recipe, almost every Recipe. want use function package sporadically, can also reference function package::function() syntax access function without load library() function. Second, Recipe many code chunks return tabular data (data.frame tibble). default, tabular format look like : output tabular data show pretty, human-readable format across entire document can also add df_print: kable attribute-value pair front-matter output document type. Now output code chunk returns data.frame tibble appear like :","code":"```{r setup, message=FALSE} # Load packages library(tidyverse) # for general data manipulation ``` # Load packages library(tidyverse)  # for general data manipulation slice_head(iris, n = 5) ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1          5.1         3.5          1.4         0.2  setosa ## 2          4.9         3.0          1.4         0.2  setosa ## 3          4.7         3.2          1.3         0.2  setosa ## 4          4.6         3.1          1.5         0.2  setosa ## 5          5.0         3.6          1.4         0.2  setosa --- title: \"Recipe #3\" author: \"Jerid Francom\" date: \"9/3/2021\" output:    html_document:      df_print: kable --- slice_head(iris, n = 5)"},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"working-with-data","dir":"Articles","previous_headings":"","what":"Working with data","title":"3. Reading, inspecting, and writing data","text":"setup place, let’s now turn reading, inspecting, writing data! Recipe focus working datasets, , data relational, tabular format.1","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"reading-data","dir":"Articles","previous_headings":"Working with data","what":"Reading data","title":"3. Reading, inspecting, and writing data","text":"Datasets can read R session assigned object various sources: package data, local data, remote data. Let’s look turn.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"package-data","dir":"Articles","previous_headings":"Working with data > Reading data","what":"Package data","title":"3. Reading, inspecting, and writing data","text":"packages either include data combination set functions may dedicated solely making data accessible. list data packages (currently) installed can accessed running data() R console. want explore data within particular package can run function argument package = name package quotes. Figure 1 see datasets included tadr package (Text Data Resources package). Figure 1: Viewing datasets tadr package. ’ve identified package data like explore, can either load package library() enter name data object use package::dataset-name convention skip loading package R session. either case, always good idea find data outputting object contents R Markdown document R Console. can simply adding ? object name bring R documentation data. Either approach bring R documentation ‘Help’ pane RStudio, seen Figure 2. Figure 2: R documentation brown dataset tadr package. Looking brown dataset R documentation, see simply call brown R Console knitting R Markdown document, get table 223,506 rows! loaded library contains data, can just use object’s name access data. case explored data, without loading package (package::dataset-name), often best now load library direct acess data data documentation R.","code":"data()  # view datasets for all installed packages data(package = \"tadr\")  # view datasets for the tadr package library(tadr) # load the package into the R session ?brown # view the R documentation ?tadr::brown # view the R documentation, without loading the package into the R session"},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"local-data","dir":"Articles","previous_headings":"Working with data > Reading data","what":"Local data","title":"3. Reading, inspecting, and writing data","text":"Packages often contain datasets demonstration purposes, datasets used research often stored computer (cloud computer!). data considered ‘local’. many formats storing local data. start accessible format tabular data delimited-text file. type dataset format delimiter typically comma tab space giving common delimited-text files names: comma-separated values (csv) tab-separated values (tsv). delimiter marks columns (variables) new lines denote new rows (observations). CSV file example TSV file example file formats can become difficult human read native format, read memory programming language piece software, human-friendly. advantage working delimited-text files plain-text files means easy share widely supported programming languages (R, Python, etc.) open-source (OpenOffice) proprietary software (MS Office). read local csv tsv file R? multitude ways read delimited-text files R session. use aptly named read_csv() read_tsv() functions readr package (Wickham, Hester, Bryan 2022), part tidyverse meta package (Wickham et al. 2019) already loaded earlier recipe! inspect functions (using ? function name R Console), see one required argument, file. value file argument path file want read. path can understood file’s address. default working R Markdown document, file’s path directory structure relative R Markdown document saved. , graphically show directory structure recipe, see path recipe_3.Rmd inaugCorpus.csv file : data/csv/inaugCorpus.csv. Let’s add path value file argument read_csv() function assign result inaug_corpus. ’s need read delimited-text file R. contents file now contained inaug_corpus object.","code":"column_1,column_2,column_3 value,value,value value,value,value column_1    column_2    column_3 value   value   value value   value   value ├── recipe_3.Rmd └── data/     ├── csv/     │   └── inaugCorpus.csv     └── tsv/         └── dailsample.tsv # read local 'inaugCorpus.csv' file inaug_corpus <- read_csv(file = \"data/csv/inaugCorpus.csv\")"},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"remote-data","dir":"Articles","previous_headings":"Working with data > Reading data","what":"Remote data","title":"3. Reading, inspecting, and writing data","text":"addition reading delimited-text files locally, can also read files stored internet. need web path, otherwise known URL (Uniform Resource Locator). Datasets delimited-text files (data storage format files) can commonly found data repository sites dedicated storing data making accessible (research) community. Let’s take example ACTIV-ES Corpus available GitHub. corpus available corpus wordlist forms. , example, want read word frequency list corpus can find copy link aes1grams.csv file. use read_csv() function URL value file argument. Now contents aes1grams.csv file contained R session aes_unigrams object.","code":"# read the 'aes1grams.csv' file from github aes_unigrams <- read_csv(file = \"https://github.com/francojc/activ-es/raw/master/activ-es-v.02/wordlists/plain/aes1grams.csv\")"},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"inspecting-data","dir":"Articles","previous_headings":"Working with data","what":"Inspecting data","title":"3. Reading, inspecting, and writing data","text":"point able read datasets R session packages local remote delimited-text files. next step inspect datasets. package datasets, already saw can use ? operator get overview dataset viewing R documentation. package data data read local remote files nothing beats getting data inspecting data . see objects read memory use ls() function. brown dataset tadr package appear load R session. load tadr package call data() function brown argument. Now run ls() brown dataset appear object session. Objects always type (Grolemund 2021). find object type class, can use class() function object name argument. Now three objects see indicate ‘data.frame’, ‘tbl’, ‘tbl_df’ aes_unigrams inaug_corpus objects also spec_tbl_df. key us recognize objects ‘data.frame’ also particular type data frame called ‘tibble’ (tbl_df). Functions readr package (part tidyverse meta-package) read rectangular data (tables) memory tibbles. Tibbles extra properties extend standard data.frame type, purposes moment let’s consider synonymous object types. Another aspect like inspect tibble objects structure, namely columns, column types, number rows. glimpse() function accomplishes . Looking output glimpse() see brown object five columns 1 million rows. Looking columns see names also see vector type column. object columns character vectors <chr>. Let’s take look aes_unigrams object fashion. information given, now see majority columns type <dbl> stands ‘double’, special type numeric vector allows decimal points. Inside RStudio objects R session information (class, structure, memory size, etc.) can also found ‘Environment’ pane. want see dataset tabular format, limit number rows print (either R Markdown document R Console), can ‘slice’ data one set functions dplyr package (Wickham et al. 2022) prefix slice_. Let’s use slice_head() get first 10 rows brown dataset. examples slice_ slice_tail() used get last n rows slice_sample() used get random sample size. Now might want explore values particular column get better understanding dataset. distinct() function can help . Note syntax used distinct(brown, category_description) can also expressed using pipe operator %>%. pipe helpful code becomes complex. , output one function becomes input next function. RStudio can use keyboard shortcut SHIFT+CMD+M insert %>% operator. Now back inspection. can also use combination group_by() count() achieve something similar distinct() function, also provide number rows distinct value. Since row represents word documents corpus, effect calculating number words category Brown corpus! Furthermore, can build last code chunk arrange output values n sorted. addition, let’s assign output new object called brown_categories. Now check R session object list see brown_categories also appears. now glimpse() structure brown_categories see new attribute Groups, tells us tibble holding group_by() function. cases best remove grouping grouping applied. use ungroup() function done operations needed grouping. can part process assigned object create call object part another code phrase. Let’s go one step group two columns, count, arrange output sort category_description n (descending order). assigning output new object brown_categories_pos, call object, remove grouping, print first 10 lines. output shows common pos (Part Speech) category. Pretty cool. Another operation can prove useful explore dataset filter() function, suggests. Using brown_categories_pos object, let’s filter pos column ‘NN’ (nouns), give us rows value pos NN. now see ‘LEARNED’ category nouns categories ‘FICTION: SCIENCE’ least. filter() takes logical expression. character vectors values quoted \"value\" numeric type values, values quoted. Let’s touch one useful function exploring datasets: select(). function also suggests, allows us select columns (subset) dataset. addition selecting columns, select() can used rename columns selected. set functions R programming toolbox can quite thorough inspection datasets.","code":"ls()  # list objects ## [1] \"aes_unigrams\" \"inaug_corpus\" library(tadr)  # load tadr package data(brown)  # read brown dataset into our R session ls()  # list objects ## [1] \"aes_unigrams\" \"brown\"        \"inaug_corpus\" class(aes_unigrams) ## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" class(brown) ## [1] \"tbl_df\"     \"tbl\"        \"data.frame\" class(inaug_corpus) ## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" glimpse(brown)  # get info on the structure of a data.frame or tibble ## Rows: 1,155,866 ## Columns: 5 ## $ document_id          <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"… ## $ category             <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",… ## $ category_description <chr> \"PRESS: REPORTAGE\", \"PRESS: REPORTAGE\", \"PRESS: R… ## $ words                <chr> \"The\", \"Fulton\", \"County\", \"Grand\", \"Jury\", \"said… ## $ pos                  <chr> \"AT\", \"NP\", \"NN\", \"JJ\", \"NN\", \"VBD\", \"NR\", \"AT\", … glimpse(aes_unigrams)  # get info on the structure ## Rows: 80,787 ## Columns: 10 ## $ ...1    <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… ## $ word    <chr> \"a\", \"á\", \"aa\", \"aaaaaa\", \"aaaaaaaa\", \"aaaaaaaaaaaaaaaaaaaaaah… ## $ ar_orf  <dbl> 2654.57, 0.24, 0.36, 0.12, 0.36, 0.12, 0.12, 0.12, 0.12, 0.24,… ## $ es_orf  <dbl> 2541.01, 0.10, 0.10, 0.00, 0.00, 0.00, 0.48, 0.00, 1.05, 0.00,… ## $ mx_orf  <dbl> 2537.99, 0.13, 0.13, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,… ## $ aes_orf <dbl> 2576.18, 0.15, 0.19, 0.04, 0.11, 0.04, 0.23, 0.04, 0.46, 0.08,… ## $ ar_ord  <dbl> 9.92, 0.16, 0.16, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.… ## $ es_ord  <dbl> 10.00, 0.05, 0.05, 0.00, 0.00, 0.00, 0.16, 0.00, 0.16, 0.00, 0… ## $ mx_ord  <dbl> 10.00, 0.08, 0.08, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… ## $ aes_ord <dbl> 9.98, 0.09, 0.09, 0.02, 0.02, 0.02, 0.09, 0.02, 0.09, 0.02, 0.… slice_head(brown, n = 10) distinct(brown, category_description)  # get the distinct values for 'category_description' brown %>% # dataset   distinct(category_description) # get distinct values for 'category_description' brown %>% # dataset   group_by(category_description) %>% # group by 'category_description'   count() # count the rows with each grouped value brown_categories <-    brown %>% # dataset   group_by(category_description) %>% # group by 'category_description'   count() %>%  # count the rows with each grouped value   arrange(n) # sort output by 'n'  brown_categories ls() ## [1] \"aes_unigrams\"     \"brown\"            \"brown_categories\" \"inaug_corpus\" glimpse(brown_categories) ## Rows: 15 ## Columns: 2 ## Groups: category_description [15] ## $ category_description <chr> \"FICTION: SCIENCE\", \"HUMOR\", \"RELIGION\", \"PRESS: … ## $ n                    <int> 14354, 21519, 39356, 40622, 56240, 61404, 67959, … brown_categories_pos <-    brown %>% # dataset   group_by(category_description, pos) %>% # group 'category_description' and 'pos'   count() %>% # count grouped columns   arrange(category_description, desc(n)) # sort by 'category_description' and descending 'n'  brown_categories_pos %>% # dataset   ungroup() %>% # remove groups   slice_head(n = 10) # first 10 rows brown_categories_pos %>% # dataset   ungroup() %>% # remove groupings   filter(pos == \"NN\") %>% # filter 'pos' for 'NN' values   arrange(desc(n)) # sort by 'n' in descending order brown_categories_nouns <-    brown_categories_pos %>% # dataset   ungroup() %>% # remove groupings   filter(pos == \"NN\") %>% # filter 'pos' for 'NN' values   arrange(desc(n)) # sort by 'n' in descending order  brown_categories_nouns %>%    select(category_description, n) brown_categories_num_nouns <-    brown_categories_nouns %>% # dataset   select(category_description, num_nouns = n) # select and rename  brown_categories_num_nouns"},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"writing-data","dir":"Articles","previous_headings":"Working with data","what":"Writing data","title":"3. Reading, inspecting, and writing data","text":"working datasets, either inspect transform , objects create along way can useful keep later use leave R session. make objects available us read another R session, two main options: write object (1) plain-text file (.csv, .tsv, etc.) (2) R Storage Data (.rds) file. write plain-text file can use readr package. various functions writing CSV text-delimited files. Let’s write brown_categories_num_nouns object created CSV file using write_csv(). now see file data/csv/ directory. can also write RDS file. function write_rds(). keep file types organized, first create rds/ directory inside data/ directory. Now couple key reasons one might choose write either plain-text file RDS file. first plain-text files widely accessible anyone –require use R. RDS files, however, require use R. second concern dataset written plain-text files retain R-specific properties. continue learn transforming analyzing datasets see R-specific properties add datasets working R help facilitate working datasets. moment, however, can point one introduced recipe. ‘Group’ attribute tibble can retain written plain-text file. RDS file, however, retains dataset part R session.","code":"write_csv(brown_categories_num_nouns, file = \"data/csv/brown_categories_num_nouns.csv\") ├── recipe_3.Rmd └── data/     ├── csv/     │   └── inaugCorpus.csv     │   └── brown_categories_num_nouns.csv     └── tsv/         └── dailsample.tsv dir.create(\"data/rds\")  # create the `data/rds/` directory write_rds(brown_categories_num_nouns, file = \"data/rds/brown_categories_num_nouns.rds\") ├── recipe_3.Rmd └── data/     ├── csv/     │   └── inaugCorpus.csv     │   └── brown_categories_num_nouns.csv     ├── rds/     │   └── brown_categories_num_nouns.rds     └── tsv/         └── dailsample.tsv"},{"path":"https://lin380.github.io/tadr/articles/recipe_3.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"3. Reading, inspecting, and writing data","text":"sum recipe, covered lot ground area reading, inspecting, writing datasets. included key functions readr dplyr package, loaded R session tidyverse meta-package. many functions strategies working datasets, introduction provides scaffolding common applications. next recipe build skills presented move beyond inspecting datasets able describe summarize datasets tabular graphical forms.","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"4. Descriptive assessment of datasets","text":"Recipe build R coding skills implement descriptive assessment datasets.","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"packages","dir":"Articles","previous_headings":"Setup","what":"Packages","title":"4. Descriptive assessment of datasets","text":"First let’s load packages help us manipulate create summaries data.","code":"library(tidyverse)  # to manipulate and visualize datasets library(janitor)  # to create (cross-)tabulations library(skimr)  # to provide statistical summaries"},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"dataset","dir":"Articles","previous_headings":"Setup","what":"Dataset","title":"4. Descriptive assessment of datasets","text":"using dataset drawn Barcelona English Language Corpus (Carmen Muñoz 2007; Carme Muñoz 2006). corpus includes series tasks aim provide data effects age acquisition English foreign language (EFL). task selected “Written composition” comprises written essays students Barcelona ranging 10 17 years age. Let’s read curated dataset1.","code":"# read the `belc` dataset from an .rds file belc <- read_rds(file = \"data/rds/belc.rds\")"},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"informational-values","dir":"Articles","previous_headings":"","what":"Informational values","title":"4. Descriptive assessment of datasets","text":"R set fundamental vector types used distinct informational value types vector types associated either categorical- (character logical) continuous-types (integer double). couple things note: first, character logical vectors can re-typed complex vector type called factor. Factors allow us encode order character vectors (say ordinal variables) assign numeric values character vector allow us mathematical operations. factors work later. Second, integer doubles also called numeric vectors. difference integer doubles doubles allow decimal places, whereas integers whole numbers. belc dataset reflects summary essays students includes number word tokens (num_tokens), number word types (num_types) ratio tokens types (ttr). structure dataset (belc) seen . see overview belc dataset, character, integer double vector types data frame. Therefore dataset contains three categorical (participant_id, age_group, sex) three continuous variables (num_tokens, num_types, ttr).","code":"## Rows: 79 ## Columns: 6 ## $ participant_id <chr> \"L02\", \"L05\", \"L10\", \"L11\", \"L12\", \"L16\", \"L22\", \"L27\",… ## $ age_group      <chr> \"10-year-olds\", \"10-year-olds\", \"10-year-olds\", \"10-yea… ## $ sex            <chr> \"female\", \"female\", \"female\", \"female\", \"female\", \"fema… ## $ num_tokens     <int> 12, 18, 36, 10, 41, 13, 47, 8, 84, 53, 18, 16, 7, 24, 3… ## $ num_types      <int> 12, 15, 26, 8, 23, 12, 30, 8, 34, 34, 13, 9, 5, 18, 23,… ## $ ttr            <dbl> 1.000, 0.833, 0.722, 0.800, 0.561, 0.923, 0.638, 1.000,…"},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"summaries","dir":"Articles","previous_headings":"","what":"Summaries","title":"4. Descriptive assessment of datasets","text":"Now let’s look approaching descriptive summaries using belc dataset. First look single vector summaries look multiple vector summaries. type summary apply depend type values variable contains, whether values categorical continuous. prepare work categorical data, let’s re-type categorical variables (participant_id, age_group sex) factors. apply .factor() function character vectors make use mutate_if() function allows target vectors dataset type character. Since age_group variable ordinal explicitly encode order. , mutate() function called targets age_group vector. Now let’s take look dataset. Now factors instead character vectors categorical variables.","code":"belc <-    belc %>%    mutate_if(is.character, as.factor) %>% # create factors from character variables   mutate(age_group = factor(age_group, ordered = TRUE)) # create ordered variable for age_group glimpse(belc)  # dataset overview ## Rows: 79 ## Columns: 6 ## $ participant_id <fct> L02, L05, L10, L11, L12, L16, L22, L27, L28, L29, L41, … ## $ age_group      <ord> 10-year-olds, 10-year-olds, 10-year-olds, 10-year-olds,… ## $ sex            <fct> female, female, female, female, female, female, female,… ## $ num_tokens     <int> 12, 18, 36, 10, 41, 13, 47, 8, 84, 53, 18, 16, 7, 24, 3… ## $ num_types      <int> 12, 15, 26, 8, 23, 12, 30, 8, 34, 34, 13, 9, 5, 18, 23,… ## $ ttr            <dbl> 1.000, 0.833, 0.722, 0.800, 0.561, 0.923, 0.638, 1.000,…"},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"single-vector","dir":"Articles","previous_headings":"Summaries","what":"Single vector","title":"4. Descriptive assessment of datasets","text":"Categorical get descriptive summary categorical variables use skimr package (Waring et al. 2022) call skim() function pipe results yank() target factor variables. Variable type: factor output see host descriptive information categorical variables. important note categorical variables summarized counts. common value categorical variable called mode. want look specific variable, say age_group can use tabyl() function janitor package (Firke 2021). see tabyl() function provides counts also proportions values age_group variable. Note working factors values often called ‘levels’. can see variable two levels count. characterizes called bimodal distribution two frequent levels. Tabular summaries often effective way assess categorical variables, let’s set stage working plotting R. Among package, tidyverse package load ggplot2 package (Wickham et al. 2021) powerful package creating plots R. ‘gg’ ggplot2 refers use “Grammar Graphics” approach building plots. three basic elements ggplot2 plots: (1) data, (2) mappings, (3) geometries. data , course, dataset want use. mappings used select variables used plot variables mapped plotting space. Finally geometries specify mappings organized.2 Let’s create simple plot age_group variable. First pass datset ggplot(). ggplot() function requires provide aesthetic mappings aes() case one variable want variable appear x axis. geom_bar() function default count levels age_group factor variable plot y axis. go. particularly informative plot, given looking single categorical variable, build basic formula creat informative graphics. Note equivalent %>% building ggplot2 plots + operator. can confusing, important recognize distinction easily overlooked can cause unexpected errors. Continuous Now let’s turn continuous variables. tabular summaries categorical variables makes sense, case continuous variables definition continuous variable count-based rather values range along continuum. Let’s look basic descriptives continuous variables skim() (time selecting numeric variables). Variable type: numeric see type summary information count-based, rather new set descriptives. mean sd (standard deviation) easy identify straightforward. summaries prefixed p represent percentiles, case five percentile points (0, 25, 50, 75, 100), slice percentile space four ranges therefore call quartiles. values often called ‘five-number summary’. 50th percentile also known median. five number summary provides numerical view distribution continuous variable. Another use quartiles. calculate range 25th 75th quantile, (50% values), known Interquartile Range (IQR). gives us precise estimate distribution include extreme values (IQR). can calculate measure manually, just apply IQR() function. categorical variables tabular formats often informative way understand variable, continuous variables plots informative. Let’s now create plots provide views distribution num_types variable. Picking quantiles, can create Empirical Cumulative Distribution Frequency (ECDF) plot give us understanding proportions values along continuous percentile range. can graphically inspect points intersect x y axis estimate percentile values number unique words. can also get specific determination using following functions. Let’s say want know many number unique words lower 10% written compositions BELC. Now let’s move towards looking distributions. done creating either histogram density plot. Let’s plot . assign plot object output side--side using gridExtra package (Auguie 2017)  can see, histogram also provides count information, like saw categorical variables. However, counts based binned groups, , range values calculated span entire value space values fall within one bins (ranges) counted. size bin range can adjusted, ’ve just gone default (case bins = 30). density plot uses proportions provide continuous view distribution. plot types advantages. case histograms can easier identify outliers density plots can help us determine easily distribution normal skewed (left right). see, identifying outliers determining type distribution working useful downstream certain types analysis approaches. topic normal distributions, let’s look useful plot assessing extent continuous variable normally distributed –Quantile-Quantile plots (QQ Plot). QQ-plots points diverge line, less likely distribution normal.","code":"belc %>%     skim() %>%     yank(\"factor\") belc %>%     tabyl(age_group)  # create a tabulation of the `age_group` variable belc %>% # dataset   ggplot(aes(x = age_group)) + # map age_group to x   geom_bar() # create a barplot belc %>%     skim() %>%     yank(\"numeric\") IQR(belc$num_types)  # calculate iqr for `num_types` ## [1] 32 belc %>% # dataset   ggplot(aes(x = num_types)) + # map `num_types` to x   stat_ecdf(geom = \"step\") # generate the cumulative distribution ecdf(belc$num_tokens) %>% # calculate the cumulative distribution   quantile(.1) # pull the value for the 10th percentile ##  10%  ## 15.4 p1 <-    belc %>% # dataset   ggplot(aes(x = num_types)) + # map `num_types` to x   geom_histogram() # create histogram  p2 <-    belc %>% # dataset   ggplot(aes(x = num_types)) + # map `num_types` to x   geom_density() # create density plot  gridExtra::grid.arrange(p1, p2, ncol = 2) # arrange both plots in two columns belc %>% # dataset   ggplot(aes(sample = num_types)) + # map `num_types` to sample   stat_qq() + # calculate the sample and theoretical quantiles points   stat_qq_line() # plot the theoretical line"},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"multiple-vector","dir":"Articles","previous_headings":"Summaries","what":"Multiple vector","title":"4. Descriptive assessment of datasets","text":"Now turn attention working multiple variables. first look variables type, look describing variables distinct types. Categorical Just descriptions single categorical variables, tabular summaries useful. multiple categorical variables, cross-tabulate. , values one variable tabulated value variable(s). Let’s cross-tabulation age_group sex variables. use tabyl() function time two variables. can also add proportions cross-tabulation adding adorn_percentages() function. ’ve also rounded output adorn_rounding(). functions part janitor package. order variables tabyl() function can allow rotate output. may desirable depending number levels particular categorical variable. Now can visualize relationship bar plot well. Let’s create two bar plots, fact. One counts second proportions.  proportions provide apples--apples comparison allowing us see relative sizes age group levels level sex. ’ve added another function labs() plot change y axis label custom label. labs() function can change labels axis well mapping aesthetics well title. Continuous summaries continuous variables can generate correlation statistics well visualize relationships. Let’s start building plot visualize relationship num_tokens ttr. plot points continuous variables coincide use geom_point() function. want include trend line use geom_smooth() function. want trend line linear, argument method = \"lm\" included.  calculate statistical summary relationship two continuous variables (correlation) can use cor() function base R’s stats package. select variables want explore assign x y select appropriata method correlation assessment. normally distributed continuous variables, set method = ‘pearson’ non-normal distributions ‘kendall’. Correlation statistics range 1 -1. closer either values means relationship strong. value closer 0 means correlation weak 0 near 0 correlation. Remember determine continuous variable conforms normal distribution can apply Shapiro-Wilk Normality Test using shapiro.test() function. significant \\(p\\)-value means distribution normal. Mixed final scenario one interested assessing relationship categorical variable continuous variable. can perform summary using group_by() summarise() functions. First group dataset categorical variable create new variable result summary statistic want calculate. case, let’s look mean number tokens level learner age group. statistic(s) want calculate us can create multiple statistics adding functions summarise() function. numeric summaries helpful reporting, visual can much easier interpret. assess categorical variable continuous variable turn box plots.","code":"belc %>% # dataset   tabyl(age_group, sex) # cross-tab of `age_group` and `sex` belc %>% # dataset   tabyl(age_group, sex) %>%  # cross-tab of `age_group` and `sex`   adorn_percentages() %>% # add percentages (row by default)   adorn_rounding(2) # round the output belc %>% # dataset   tabyl(sex, age_group) %>%  # cross-tab of `age_group` and `sex`   adorn_percentages() %>% # add percentages (row by default)   adorn_rounding(2) # round the output p1 <-    belc %>% # dataset   ggplot(aes(x = sex, fill = age_group)) + # map sex to x and age_group to y   geom_bar() + # generate bar plot with counts   labs(y = \"Count\") # add labels  p2 <-    belc %>% # dataset   ggplot(aes(x = sex, fill = age_group)) + # map sex to x and age_group to y   geom_bar(position = \"fill\") + # generate bar plot with proportions   labs(y = \"Proportion\") # add labels  gridExtra::grid.arrange(p1, p2, ncol = 2) # arrange both plots in two columns p1 <-    belc %>% # dataset   ggplot(aes(x = num_tokens, y = ttr)) + # map num_tokens to x, ttr to y   geom_point() + # plot x/y points   labs(x = \"Number of tokens\", y = \"Type-Token Ratio\") p2 <-    belc %>% # dataset   ggplot(aes(x = num_tokens, y = ttr)) + # map num_tokens to x, ttr to y   geom_point() + # plot x/y points   geom_smooth(method = \"lm\") + # add a linear trend line   labs(x = \"Number of tokens\", y = \"Type-Token Ratio\")  gridExtra::grid.arrange(p1, p2, ncol = 2) # arrange both plots in two columns cor(x = belc$num_tokens, y = belc$ttr, method = \"kendall\")  # correlation stat ## [1] -0.563 belc %>% # dataset   group_by(age_group) %>% # group dataset by age_group   summarise(mean_num_tokens = mean(num_tokens)) # calculate the mean num_tokens belc %>% # dataset   group_by(age_group) %>% # group dataset by age_group   summarise(mean_num_tokens = mean(num_tokens), # calculate mean             sd_num_tokens = sd(num_tokens), # calculate standard deviation              median_num_tokens = median(num_tokens), # calculate median             iqr_num_tokens = IQR(num_tokens)) # calculate the interquartile range score belc %>% # dataset   ggplot(aes(x = age_group, y = num_tokens)) + # map age_group to x and num_tokens to y   geom_boxplot() # create box plot"},{"path":"https://lin380.github.io/tadr/articles/recipe_4.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"4. Descriptive assessment of datasets","text":"recipe covered various common strategies descriptively assessing variables dataset. worked single variables categorical continuous types discussing relationship R’s vector types informational values well looking descriptive stats visualizations. also looked strategies assessing multiple variables either type mixed.","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"Recipe, turning attention getting familiar software resources used share collaborate research projects. using Git GitHub manage, store, publish projects.1","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"creating-a-github-account","dir":"Articles","previous_headings":"Getting setup with Git and Github","what":"Creating a GitHub account","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"GitHub signup. Note consideration may want take account setting GitHub account. Note, want use university email like later take advantage Student Education Benefits.  created account, presented following page. Click ‘Continue’ button modify repositories main README.md file.  editing README.md file, skip bottom page can add comment click ‘Commit new file’.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"create-a-new-repository","dir":"Articles","previous_headings":"Getting setup with Git and Github","what":"Create a new repository","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"navigate repository listings. Click ‘New’ start process creating new repository.  Give new repository name test_repo, provide short description, make sure repository ‘Public’, check ‘Add README file’. skip bottom page click ‘Create repository’.  presented page repository accessible URL. follow steps page. Rather copy URL page navigate RStudio Cloud. ### Create RStudio Cloud Project repository ‘Workspace’ RStudio Cloud, click ‘New Project’ select ‘New Project Git Repository’. copy GitHub repository test_repo RStudio Cloud new R Project.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"setting-up-git-on-rstudio-cloud","dir":"Articles","previous_headings":"Getting setup with Git and Github","what":"Setting up Git on RStudio Cloud","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"able make changes project RStudio Cloud send changes back GitHub first need set Git RStudio Cloud. Git engine behind GitHub already installed default RStudio Cloud projects. make process configuring Git talk GitHub needs know pieces information: (1) GitHub username (user.name), (2) GitHub email address (user.email), (3) password GitHub. make setup easier steps 1 2, install usethis package (Wickham, Bryan, Barrett 2021). Select ‘Packages’ pane click ‘Install’. ‘Packages’ field interactive dialogue box, enter usethis click ‘Install’. usethis package installed > prompt available R Console, load package enter GitHub configuration details (user.name user.email) use_git_config() function.  confirm configuration details entered use_git_config() function registered Git, move ‘Terminal’ pane (just right Console pane) enter command git config --global --list (note double hyphens!). see ‘user.name’ ‘user.email’ set credentials.  final step configure Git talk GitHub create GitHub token store Git configuration RStudio Cloud. token effect password. usethis package’s create_github_token() function open interactive session GitHub can create token, known personal access token, PAT. Console run create_github_token() (arguments). browser tab open ‘New personal access token’ page. required field ‘Expiration’ field. Set ‘90 days’. skip bottom page create token. Copy store token (PAT) shown screen safe place need just bit (every time want set Git/ GitHub new RStudio Cloud project). Now navigate back RStudio Cloud test_repo project ’ve working . add PAT Git configuration going install another R package make process easy . installing usethis, navigate ‘Packages’ pane click ‘Install’ enter gitcreds (Csárdi 2020) dialogue box click ‘Install’ install package. Load gitcreds package run gitcreds_set() function. prompted enter PAT. Return GitHub tab browser PAT showing copy token. Return RStudio Cloud paste PAT ‘? Enter password token’ prompt hit ‘Return’ keyboard. PAT registered Git RStudio Cloud feedback given. ensure PAT registered, can use gitcreds_get() function. well see ‘username’ ‘PersonalAccessToken’ ‘password’ ‘<– hidden —>’. Now Git configuration setup talk Github!","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"project-workflow","dir":"Articles","previous_headings":"","what":"Project workflow","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"Now ’ve connected Git GitHub repository RStudio Cloud can now demonstrate use Git log changes make project sync back GitHub.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"making-changes","dir":"Articles","previous_headings":"Project workflow","what":"Making changes","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"point Git tracking changes make project. includes changes files, addition new files, deletion files. testing purposes, let’s open ‘README.md’ file project make simple change. case just typed ‘Hello world!’. Now navigate ‘Terminal’ tab enter command git status. return current status files Git tracking. Skipping details output, let’s focus couple things. First see ‘README.md’ file listed ‘modified’ (red). makes sense, ’ve just added ‘Hello world!’ text file. ‘README.md’ file GitHub longer date current status file RStudio Cloud project. Second Git paying attention new files well. see files ‘.gitignore’ ‘project.Rproj’ tracked (also appear red).","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"adding-and-committing-changes","dir":"Articles","previous_headings":"Project workflow","what":"Adding and committing changes","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"add untracked files tracked status run git add -command ‘Terminal’ pane. “stages” files part Git registry. next step “commit” changed new files Git log running git commit -m \"<informative message.>\". message gave basic, just note state first commit project.  log basically list snapshots project segment saved points, “lines sand”. commit snapshot requires brief message describe done. commit snapshots can helpful like revert project back status project one points. now, however, key note send GitHub changes unless committed Git.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"pushing-changes-to-github","dir":"Articles","previous_headings":"Project workflow","what":"Pushing changes to GitHub","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"send committed changes GitHub? Well, ‘push’ GitHub git push command. point changes made, files added, registry commits created synced GitHub repository. Navigate back test_repo repository page GitHub refresh browser page. now see updates project reflected GitHub.  can now see updates, message(s) associated commits, time since commit pushed GitHub. now navigate main page GitHub profile, see repository among list repositories associated GitHub account.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"fork-a-repository","dir":"Articles","previous_headings":"","what":"Fork a repository","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"can find project create copy project GitHub account. process known ‘forking’. can find demonstration repository (demo_repo) ’ve created, seen .  Click ‘Fork’ top left GitHub repository’s page. taken copy repo.  forked repository account, can create new RStudio Cloud project using URL forked copy repository.  Since RStudio Cloud project starts ‘fresh’, packages installed previous RStudio Cloud project need installed new project steps set Git configuration talk GitHub need followed. ’s summary: R Packages Install R packages ‘usethis’ ‘gitcreds’. Load ‘usethis’ ‘gitcreds’ packages. Git credentials Set GitHub username user email use_git_config(user.name = \"<-username>\", user.email = \"<-email>\"). Set GitHub personal access token (PAT) gitcreds_set(). Copy paste PAT prompt hit ‘Return’. can check user name email configuration settings running git config --global --list Terminal pane.  check PAT R Console running gitcreds_get().  now ready edit, add, / delete files project! workflow adding, committing, pushing changes GitHub , summary steps. can periodically run git status find status Git relation changes project. usually run beginning workflow git add -see ready commit.","code":"git add -A git commit -m \"<a short message describing what you've done.>\" git push"},{"path":"https://lin380.github.io/tadr/articles/recipe_5.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"5. Project management with Git, GitHub, and RStudio Cloud","text":"Recipe introduced Git, GitHub, set technologies work RStudio Cloud manage, store, publish research projects. also seen fork programmers’ GitHub repositories set can work . way research published GitHub contributes reproducible research researcher can access reproduce / extend another researcher’s work!","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"6. Control statements, custom functions, and iteration","text":"Recipe, dive R strategies increase ability produce effective, concise, reproducible code. three main areas cover working control statements, writing custom functions, leveraging iteration. programming strategies often useful acquiring data , see, powerful concepts can used throughout reproducible research project. illustrate usefulness R coding strategies provide two data collection cases: One using rtweet pacakge collect data Twitter second using rvest package perform webscraping. get started let’s load main packages work recipe.","code":"library(tidyverse)  # data manipulation library(rtweet)  # collecting tweets from Twitter API library(rvest)  # webscraping library(purrr)  # iteration"},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"coding-strategies","dir":"Articles","previous_headings":"","what":"Coding strategies","title":"6. Control statements, custom functions, and iteration","text":"section discuss control statments, custom functions, iteration providing general understanding work develop effective efficient R coding practices.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"control-statements","dir":"Articles","previous_headings":"Coding strategies","what":"Control statements","title":"6. Control statements, custom functions, and iteration","text":"programming general, R particular, often case points code programmer want code make decisions process (process) based state current code. want make choice point need frame operation returns logical (TRUE/FALSE) output. example, let’s set variable x 1. ask R evaluate x. applies character values. operation returns logical value can used. example used relational operators, function returns logical value can used . set functions come stringr package allow us evaluate character values various ways. Let’s evaluate whether y starts letter “D”. functions return logical values can reverse logical value prefixing function !. mind can control code flow () function. basic function control code execute , use () function. function evaluates whether operation returns TRUE FALSE –TRUE code executes, . Let’s make simple piece code returns message character vector starts “D”. Inside () function add logical operation. add opening closing braces tell code execute logical operation returns TRUE. case code runs vector element starts “D”. sometimes want execute code () operation returns FALSE. else condition comes . Let’s change value y “Judieth”. Note else condition opens closes new set braces wrap alternative condition. effectively makes code dynamic adjusting particular values given object.","code":"x <- 1  # set x to 1 x == 1  # evaluate x is equal to 1 ## [1] TRUE x != 1  # evaluate x is not equal to 1 ## [1] FALSE y <- \"Delwin\"  # set y to Delwin y == \"Delwin\"  # evaluate y is equal to Delwin ## [1] TRUE y != \"Delwin\"  # evaluate y is not equal to Delwin ## [1] FALSE str_starts(y, \"D\")  # evaluate y starts with D ## [1] TRUE str_starts(y, \"J\")  # evaluate y starts with J ## [1] FALSE !str_starts(y, \"J\")  # evaluate y starts with J, reversed ## [1] TRUE if (str_starts(y, \"D\")) {     # if evaluate y starts with J is TRUE     paste(\"This name starts with 'D'\")  # if true, print message } ## [1] \"This name starts with 'D'\" y <- \"Judeith\"  # set y to Judeith if (str_starts(y, \"D\")) {     # if evaluate y starts with D is TRUE     paste(\"This name starts with 'D'\")  # if true, message } else {     paste(\"This name does not start with 'D'\")  # if false, print message } ## [1] \"This name does not start with 'D'\""},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"custom-functions","dir":"Articles","previous_headings":"Coding strategies","what":"Custom functions","title":"6. Control statements, custom functions, and iteration","text":"Now like use code code copy paste need . may even adjust change logical evaluation expression similar, distinct situation. ideally able reuse code way provides us similar functionality flexibility. writing custom function comes handy. custom function simply way reuse code make code easily accessible. ’s basic form creating custom function includes calling function() function assigning object name. code run function wrapped braces. code want run inside function depends variable variables, include inside function() function. names variables included code values assigned passed code. variables known arguments. Now can assign value argument my_function() function anything want reflected code run. function can arguments multiple arguments, needed. Furthermore, function’s arguments can default values. Let’s rewrite my_function() function include two arguments, one default value. ’m also going make argument names bit meaningful use transparent rename function meaningful. argument default value specified function creation . Since default value need specified use function. , however, want value default, need assign argument new value. Let’s apply knowledge custom functions code /else example previous section. Since function want used similar situation, still flexibility look like maintain code aspects like depend values pass function. think makes sense name letter arguments. ’ll set letter argument default “D”. Note ’ve added argument name message paste() function. two points consider creating using custom functions code projects. First, plan use function distinct parts project, distinct R scripts R Markdown documents, want add function separate R script source script inside code make function accessible. Let unpack bit. notice create custom function, must run custom function make accessible code. appear ‘Environment’ pane RStudio subsection ‘Functions’. objects (functions) ‘Environment’ pane session-specific, restart R, ‘Environment’ cleared. able access function anywhere want, create new R script (.R) copy function script. often create directory specifically files functions. use source() function run function.R script, contains functions, effectively adding functions stored current R session. Note relative path functions.R file argument source(). point personal preference whether create individual R scripts custom function create, group multiple functions one R script. Second, invoke function package, say stringr package case, important package either loaded function run reference function made explicit (:: convention). use various functions package, worthwhile load package library() beginning custom function. Alternatively, can use :: convention avoid loading entire package.","code":"my_function <- function() {     # some code to run } my_function <- function(argument) {     paste(argument)  # print message } my_function(argument = \"Hi\")  # print `argument` ## [1] \"Hi\" my_function(argument = \"Hi Jerid\")  # print `argument` ## [1] \"Hi Jerid\" greetings <- function(greeting = \"Hi\", name) {     # Function: Print a greeting and name     paste(greeting, name)  # print message }  greetings(name = \"Jerid\")  # print greeting to name ## [1] \"Hi Jerid\" greetings(greeting = \"Hello\", name = \"Jerid\")  # print greeting to name ## [1] \"Hello Jerid\" name_starts_with <- function(name, letter = \"D\") {     # Function: Evaluates if the name starts with the given letter      if (str_starts(name, letter)) {         # if evaluate name starts with letter         paste(name, \"starts with\", letter)  # if true, print message     } else {         paste(name, \"does not start with\", letter)  # if false, print message     } }  name_starts_with(name = \"Jerid\")  # name starts with D message ## [1] \"Jerid does not start with D\" name_starts_with(name = \"Jerid\", letter = \"J\")  # name starts with J message ## [1] \"Jerid starts with J\" functions/ └── functions.R source(\"../functions/functions.R\") name_starts_with <- function(name, letter = \"D\") {     # Function: Evaluates if the name starts with the given letter      library(stringr)  # to detect first letter      if (stringr::str_starts(name, letter)) {         # if evaluate name starts with letter         paste(name, \"starts with\", letter)  # if true, print message     } else {         paste(name, \"does not start with\", letter)  # if false, print message     } } name_starts_with <- function(name, letter = \"D\") {     # Function: Evaluates if the name starts with the given letter      if (stringr::str_starts(name, letter)) {         # if evaluate name starts with letter         paste(name, \"starts with\", letter)  # if true, print message     } else {         paste(name, \"does not start with\", letter)  # if false, print message     } }"},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"iteration","dir":"Articles","previous_headings":"Coding strategies","what":"Iteration","title":"6. Control statements, custom functions, and iteration","text":"point ’ve seen control flow code control statements explored reuse code efficient way creating using custom functions. final coding strategy recipe concerns passing multiple arguments function function deal argument turn return results together. known iteration. Say want use name_starts_with() function pass multiple names function. One way simply apply function multiple times new name. , however, ideal. working many values code become cumbersome. able apply custom function many values time, turn map() function purrr package. First let’s names work . babynames dataset babynames package help us . ’ll random sample 10 names dataset keep name column. Use `?babynames::babynames find dataset. Let’s start seeing happens pass names sample_names vector function name_starts_with(). see warning triggered first glance still seems function worked. Take closer look, however, see name “Dontez” gets message “start D” –clearly case. happened -else statements allow multiple values passed just accepts first value, case ‘Macey’ –start ‘D’. can see clearly changing letter evaluate ‘M’, ‘Macey’ first value. warning now messages claim names start ‘M’. map() function allows us pass multiple values function value run function results returned together. Now get correct output, results map() default return list. like results character vector, just like character vector passed function. various map_*() functions specifiy type object returned . case want map_chr() give us single character vector. name_starts_with() function another possible argument, letter argument. specify new value argument, can add map() call. Note, however, argument-value pair need separated comma.","code":"name_starts_with(name = \"Dawn\") ## [1] \"Dawn starts with D\" name_starts_with(name = \"Vince\") ## [1] \"Vince does not start with D\" name_starts_with(name = \"Stephanie\") ## [1] \"Stephanie does not start with D\" library(babynames) # for the babynames dataset  set.seed(1111) # make the random sample reproducible  sample_names <-    babynames %>% # dataset   slice_sample(n = 10) %>%  # randomly sample 10 names   pull(name) # isolate the `name` vector  sample_names ##  [1] \"Macey\"   \"Tausha\"  \"Merlin\"  \"Vernel\"  \"Dontez\"  \"Nallely\" \"Anna\"    ##  [8] \"Hogan\"   \"Manola\"  \"Jakalyn\" sample_names %>%     name_starts_with() ## Warning in if (str_starts(name, letter)) {: the condition has length > 1 and ## only the first element will be used ##  [1] \"Macey does not start with D\"   \"Tausha does not start with D\"  ##  [3] \"Merlin does not start with D\"  \"Vernel does not start with D\"  ##  [5] \"Dontez does not start with D\"  \"Nallely does not start with D\" ##  [7] \"Anna does not start with D\"    \"Hogan does not start with D\"   ##  [9] \"Manola does not start with D\"  \"Jakalyn does not start with D\" sample_names %>%     name_starts_with(letter = \"M\") ## Warning in if (str_starts(name, letter)) {: the condition has length > 1 and ## only the first element will be used ##  [1] \"Macey starts with M\"   \"Tausha starts with M\"  \"Merlin starts with M\"  ##  [4] \"Vernel starts with M\"  \"Dontez starts with M\"  \"Nallely starts with M\" ##  [7] \"Anna starts with M\"    \"Hogan starts with M\"   \"Manola starts with M\"  ## [10] \"Jakalyn starts with M\" sample_names %>%     map(name_starts_with) ## [[1]] ## [1] \"Macey does not start with D\" ##  ## [[2]] ## [1] \"Tausha does not start with D\" ##  ## [[3]] ## [1] \"Merlin does not start with D\" ##  ## [[4]] ## [1] \"Vernel does not start with D\" ##  ## [[5]] ## [1] \"Dontez starts with D\" ##  ## [[6]] ## [1] \"Nallely does not start with D\" ##  ## [[7]] ## [1] \"Anna does not start with D\" ##  ## [[8]] ## [1] \"Hogan does not start with D\" ##  ## [[9]] ## [1] \"Manola does not start with D\" ##  ## [[10]] ## [1] \"Jakalyn does not start with D\" sample_names %>%     map_chr(name_starts_with) ##  [1] \"Macey does not start with D\"   \"Tausha does not start with D\"  ##  [3] \"Merlin does not start with D\"  \"Vernel does not start with D\"  ##  [5] \"Dontez starts with D\"          \"Nallely does not start with D\" ##  [7] \"Anna does not start with D\"    \"Hogan does not start with D\"   ##  [9] \"Manola does not start with D\"  \"Jakalyn does not start with D\" sample_names %>%     map_chr(name_starts_with, letter = \"M\") ##  [1] \"Macey starts with M\"           \"Tausha does not start with M\"  ##  [3] \"Merlin starts with M\"          \"Vernel does not start with M\"  ##  [5] \"Dontez does not start with M\"  \"Nallely does not start with M\" ##  [7] \"Anna does not start with M\"    \"Hogan does not start with M\"   ##  [9] \"Manola starts with M\"          \"Jakalyn does not start with M\""},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"cases","dir":"Articles","previous_headings":"","what":"Cases","title":"6. Control statements, custom functions, and iteration","text":"examples control statements, custom functions, iteration hopefully helpful illustrate use, admittedly basic. ground coding strategies acquiring text data, let’s turn working realistic cases.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"acquire-tweets","dir":"Articles","previous_headings":"Cases","what":"Acquire tweets","title":"6. Control statements, custom functions, and iteration","text":"rtweet package wide range functions allow access Twitter API. useful function search_tweets() function allows access sample Twitter posts within recent past based search query. options use ?rtweet::search_tweets explore documentation. test case going aim collect tweets various search query terms one time compile results one data frame. First let’s set basic function meet aims get started. ’ve used query term ‘uppity’, set function retrieve approximately 100 tweets matching term type tweet set recent. ’ve chosen avoid retweets set geographic boundary United States. ’ve also set Twitter API token ’m using authenticate search. query returns following set information. OK. Since going want pass various terms/ expressions function also want make sure search query added structure data frame. creating variable pass q argument add column data frame mutate(). Now can replace value search_term run search , wanted, search term added new column search_term resulting data frame. let’s go one step create custom function makes easy us run search replacing search_term. addition, add variable placeholder n custom function can change needed. Note ’ve added sQuote() around search_term search_tweet function make sure multiword terms quoted single quotes 'term expression'. tells function search whole phrase just individual words. term_search() function pretty useful makes searching along parameters set easy . want search multiple terms/ expressions time return together. search terms ’ve selected vector called bias_terms. Now can employ map() function purrr package pass term custom function term_search. ’ve set n 500, overriding default 100. Let’s take look many tweets obtained search terms. Notice obtained 500 matching tweets terms except ‘peanut gallery’. underscores guaranteed get exactly number tweets requested given time. Let’s save results disk using save_as_csv() function rtweet. reproducible research project may want make sure don’t overwrite results Twitter query time run script result distinct. can use control statement avoid conducting new search, unless desired . Just fun, let’s map tweets color points map according specific bias term. get geocoordinates lat lng variables extracted lat_lng() function.","code":"tweets <-    search_tweets(q = \"uppity\", # query term                 n = 100, # number of desired tweets                 include_rts = FALSE, # no retweets                 geocode = lookup_coords(\"usa\"), # only from US                 token = student_token) %>%  # token for authentication   lat_lng() # extract the geocoordinates where available glimpse(tweets) ## Rows: 100 ## Columns: 92 ## $ user_id                 <chr> \"1007992881531969536\", \"1470190191877013514\", … ## $ status_id               <chr> \"1516101406716936195\", \"1516099241298604040\", … ## $ created_at              <dttm> 2022-04-18 17:08:37, 2022-04-18 17:00:01, 202… ## $ screen_name             <chr> \"KennadyTim\", \"PaneraBabe\", \"ambientsidewalk\",… ## $ text                    <chr> \"@Jim_Jordan Because free speech = truth to po… ## $ source                  <chr> \"Twitter Web App\", \"Twitter for iPhone\", \"Twit… ## $ display_text_width      <dbl> 189, 69, 263, 14, 59, 23, 258, 142, 141, 270, … ## $ reply_to_status_id      <chr> \"1516077280929406976\", NA, \"151607434975269274… ## $ reply_to_user_id        <chr> \"18166778\", NA, \"549322049\", \"1269855100115152… ## $ reply_to_screen_name    <chr> \"Jim_Jordan\", NA, \"dropoutninja\", \"KingkaayCo3… ## $ is_quote                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… ## $ is_retweet              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… ## $ favorite_count          <int> 0, 0, 1, 1, 0, 1, 1, 0, 4, 0, 11, 1, 0, 1, 2, … ## $ retweet_count           <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ quote_count             <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ reply_count             <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ hashtags                <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ symbols                 <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ urls_url                <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"open… ## $ urls_t.co               <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"http… ## $ urls_expanded_url       <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"http… ## $ media_url               <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ media_t.co              <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ media_expanded_url      <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ media_type              <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ext_media_url           <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ext_media_t.co          <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ext_media_expanded_url  <list> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ ext_media_type          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ mentions_user_id        <list> \"18166778\", NA, \"549322049\", \"126985510011515… ## $ mentions_screen_name    <list> \"Jim_Jordan\", NA, \"dropoutninja\", \"KingkaayCo… ## $ lang                    <chr> \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\"… ## $ quoted_status_id        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"1… ## $ quoted_text             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"A… ## $ quoted_created_at       <dttm> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2… ## $ quoted_source           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"T… ## $ quoted_favorite_count   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 13… ## $ quoted_retweet_count    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 19… ## $ quoted_user_id          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"1… ## $ quoted_screen_name      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"w… ## $ quoted_name             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"S… ## $ quoted_followers_count  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 28… ## $ quoted_friends_count    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 19… ## $ quoted_statuses_count   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 37… ## $ quoted_location         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"H… ## $ quoted_description      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"I… ## $ quoted_verified         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, FA… ## $ retweet_status_id       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_text            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_created_at      <dttm> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ retweet_source          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_favorite_count  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_retweet_count   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_user_id         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_screen_name     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_name            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_followers_count <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_friends_count   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_statuses_count  <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_location        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_description     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ retweet_verified        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ place_url               <chr> NA, \"https://api.twitter.com/1.1/geo/id/018d3e… ## $ place_name              <chr> NA, \"North Tonawanda\", NA, \"Los Angeles\", NA, … ## $ place_full_name         <chr> NA, \"North Tonawanda, NY\", NA, \"Los Angeles, C… ## $ place_type              <chr> NA, \"city\", NA, \"city\", NA, NA, NA, NA, NA, NA… ## $ country                 <chr> NA, \"United States\", NA, \"United States\", NA, … ## $ country_code            <chr> NA, \"US\", NA, \"US\", NA, NA, NA, NA, NA, NA, NA… ## $ geo_coords              <list> <NA, NA>, <NA, NA>, <NA, NA>, <NA, NA>, <NA, … ## $ coords_coords           <list> <NA, NA>, <NA, NA>, <NA, NA>, <NA, NA>, <NA, … ## $ bbox_coords             <list> <NA, NA, NA, NA, NA, NA, NA, NA>, <-78.9, -78… ## $ status_url              <chr> \"https://twitter.com/KennadyTim/status/1516101… ## $ name                    <chr> \"Tim Kennady\", \"Managemento.\", \"Heath #BlackLi… ## $ location                <chr> \"California, USA\", \"Sesame street\", \"Austin, T… ## $ description             <chr> \"Polished Concrete Industry Visionary and Prov… ## $ url                     <chr> NA, NA, NA, \"https://t.co/mGinIugD1m\", NA, NA,… ## $ protected               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… ## $ followers_count         <int> 5, 38, 829, 1486, 2754, 233, 395, 23053, 14, 9… ## $ friends_count           <int> 17, 21, 280, 1148, 3177, 281, 1081, 15088, 57,… ## $ listed_count            <int> 0, 0, 14, 7, 77, 1, 11, 52, 0, 10, 56, 0, 22, … ## $ statuses_count          <int> 115, 1476, 6884, 20443, 117764, 5012, 13843, 9… ## $ favourites_count        <int> 13, 443, 8044, 120175, 91678, 6773, 27235, 285… ## $ account_created_at      <dttm> 2018-06-16 14:26:45, 2021-12-13 00:34:00, 200… ## $ verified                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… ## $ profile_url             <chr> NA, NA, NA, \"https://t.co/mGinIugD1m\", NA, NA,… ## $ profile_expanded_url    <chr> NA, NA, NA, \"https://linktr.ee/CaymenParks\", N… ## $ account_lang            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ profile_banner_url      <chr> \"https://pbs.twimg.com/profile_banners/1007992… ## $ profile_background_url  <chr> NA, NA, \"http://abs.twimg.com/images/themes/th… ## $ profile_image_url       <chr> \"http://pbs.twimg.com/profile_images/100799466… ## $ lat                     <dbl> NA, 43.1, NA, 34.0, NA, NA, NA, NA, NA, NA, NA… ## $ lng                     <dbl> NA, -78.9, NA, -118.4, NA, NA, NA, NA, NA, NA,… search_term <- \"uppity\" # create a search_term variable  tweets <-    search_tweets(q = search_term, # query term (from search_term)                 n = 100, # number of desired tweets                 include_rts = FALSE, # no retweets                 geocode = lookup_coords(\"usa\"), # only from US                 token = student_token) %>%  # token for authentication   lat_lng() %>% # extract the geocoordinates where available   mutate(search_term = search_term) # add search_term value to the data frame term_search <-    function(search_term, n = 100) {     # Function:      # Search recent tweets for specific term          library(rtweet) # to search Twitter API          tweets <-        search_tweets(q = sQuote(search_term), # query term (from search_term)                     n = n, # number of desired tweets (from n)                     include_rts = FALSE, # no retweets                     geocode = lookup_coords(\"usa\"), # only from US                     token = student_token) %>%  # token for authentication       lat_lng() %>% # extract the geocoordinates where available       mutate(search_term = search_term) # add search_term value to the data frame     return(tweets) # return the results   } # Biased language Source: # https://blog.ongig.com/diversity-and-inclusion/biased-language-examples/  bias_terms <- c(\"uppity\", \"ghetto\", \"peanut gallery\", \"call a spade a spade\") results <-    bias_terms %>% # terms to search   map(term_search, n = 500) %>% # apply the function to each term, retrieve 500 tweets (if available)   do_call_rbind() %>% # join the results by rows   return() # return the results results %>%     count(search_term, sort = TRUE) save_as_csv(results, file_name = \"../data/original/twitter/bias_terms.csv\") conduct_search <- FALSE # set to TRUE to conduct a new search  if(conduct_search) {      cat(\"Conducting new search. \\n\")      bias_terms <- c(\"uppity\", \"ghetto\", \"peanut gallery\", \"call a spade a spade\")      bias_terms %>% # terms to search     map(term_search, n = 500) %>% # apply the function to each term, retrieve 500 tweets (if available)     do_call_rbind() %>% # join the results by rows     save_as_csv(file_name = \"../data/original/twitter/bias_terms.csv\")   cat(\"Search results saved to disk. \\n\")    } else {   cat(\"Keeping previous search results. \\n\") } states_map <- map_data(\"state\")  # from ggplot2  p <- ggplot() + geom_polygon(data = states_map, aes(x = long, y = lat, group = group),     fill = \"grey\", color = \"black\") + labs(title = \"Tweets in the USA\", subtitle = \"Bias terms\")  p + geom_point(data = results, aes(x = lng, y = lat, group = 1, color = search_term),     alpha = 1/2, size = 1.5)"},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"webscrape-text","dir":"Articles","previous_headings":"Cases","what":"Webscrape text","title":"6. Control statements, custom functions, and iteration","text":"development","code":"search_url <- \"https://www.msnbc.com/search/?q=latinx#gsc.tab=0&gsc.q=latinx%20opinion&gsc.sort=date\"  search_url <- \"https://www.msnbc.com/opinion/biden-reversing-trump-s-discriminatory-housing-rulings-we-need-more-n1276857\"  search_html <- read_html(search_url)  search_html %>%     html_elements(\"div.article-body__content\") %>%     html_text() url <- \"http://www.chakoteya.net/DoctorWho/episodes13.html\"  html <- read_html(url)  html %>%     html_elements(\"a\") %>%     html_attr(\"href\") %>%     # pluck() %>% str_extract(pattern = \"^\\\\d.*.html$\") main_url <- \"http://www.chakoteya.net/DoctorWho/\"  html <- read_html(main_url)  html %>%     html_elements(\"a\") %>%     html_attr(\"href\") url <- \"https://www.nytimes.com/2021/09/21/opinion/gender-pronouns-they.html\" html <- read_html(url)  html"},{"path":"https://lin380.github.io/tadr/articles/recipe_6.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"6. Control statements, custom functions, and iteration","text":"recipe taken closer look key programming strategies applicable acquiring data, also many parts text analysis project. strategies allow programmer control code flow, create reusable custom functions iterate programming tasks. Together strategies make concise, efficient, effective. next recipe turn curating data. , process taking data dataset manipulating ’s structure conforms tidy data principles.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"7. Tidying data: regular expressions and reshaping datasets","text":"Recipe, take closer look couple coding strategies quite applicable just stage text analysis project, make particular sense curating data. look regular expressions helpful developing strategies matching, extracting, / replacing patterns character sequences change dimensions dataset either expand collapse columns rows. practical case look curating data TV subtitles. use regular expressions strategies reshape datasets. get started let’s load main packages work recipe.","code":"library(tidyverse)  # data manipulation library(readtext)  # read files library(tidytext)  # text segmentation"},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"regular-expressions","dir":"Articles","previous_headings":"Coding strategies","what":"Regular expressions","title":"7. Tidying data: regular expressions and reshaping datasets","text":"often case working text want find, extract, / replace characters character sequences (strings) data datasets. string patterns extremely simple, may need use ‘literal’ strings (exact term searching ), used typical searches may perform word processing document, example, help find occurrences string. cases, however, string patterns literal. say may want find sort meta-character string pattern single digit, sequence digits, words begin capital letters, last words line, etc. case simple type literal character sequence match patterns. However regular pattern string search syntax known regular expressions can help! Take example following text: small fragment contents subtitle file SRT (SubRip File Format) formatting. likely interested extracting language sequences removing formatting sequences, question ? Regular expressions! regular expression (regex) use various general pattern matching conventions capture abstract, regular patterns strings. common types regex operators, grouped terms function. Classes \\w letter digit character \\d digit character \\s whitespace character . single character Quantifiers x? zero one x x* zero x x+ one x x{2,} two x x{5,10} five ten x Anchors ^ start string $ end string \\b word boundary \\n new line Escape \\ make character class, quantifier, anchor, etc. literal case, want “clean” text separating non-linguistic information linguistic information. process cleaning may match pattern extract (keep) match remove replace. Looking text , looks like good first step may match remove. Specifically, let’s see can match lines start digits. Building working regular expression requires trial error. speed process, tend use interactive regex website, regex101. ’ve opened website pasted example text ‘TEST STRING’ field. Now let’s work building regex example. Let’s try adding ^ operator match beginning lines.  Nothing matched, regex101 site shows us pink vertical line ’s pattern matching begin. Now let’s add \\d digit operator.  now see lines first character digit active first digit highlighted. far good. Comparing active lines, see three six one digit end new line (⏎). add \\n operator match new line get following.  can see lost longer lines contain various characters end line. want able capture lines start digits either characters end line first digits without characters. can turn .* operator combination match character (.) zero times (*). Inserting existing expression provides desired match.  now decent regex (^\\d.*\\n) match lines like remove example text. First let read example text character vector line element vector. example_text text , now line file coverted separate vector. can concatenate vectors new line operator (\\n) show text appears . Now let’s put regular expression work. turn stringr package. Specifically function str_remove() two arguments string pattern. string example_text character vector pattern regular expression (^\\d.*\\n) –two twists. First, example_text broken separate vectors lines, new line anchors (\\n). Second, R \\ characters regexes need escaped, need appended \\. regex becomes ^\\\\d.*. works! Now may clean , example highlight practicality using regular expressions text processing. Regular expressions used many ways across programming languages, including R, well many desktop web applications –keep eyes open opportunities use regexes!","code":"1 00:00:11,160 --> 00:00:13,920 - [man] What do you think love is? - It'll be like a fairy tale.  2 00:00:14,000 --> 00:00:15,560 A natural high, I suppose?  3 00:00:15,640 --> 00:00:17,160 Gets your heart racing. example_text <-    read_lines(file = \"recipe_7/data/original/love_on_the_spectrum/Love.On.The.Spectrum.S01E01.WEBRip.x264-ION10.srt\", # path to the .srt file              n_max = 12) # read only the first 12 lines  example_text # view character vector ##  [1] \"1\"                                  \"00:00:11,160 --> 00:00:13,920\"      ##  [3] \"- [man] What do you think love is?\" \"- It'll be like a fairy tale.\"      ##  [5] \"\"                                   \"2\"                                  ##  [7] \"00:00:14,000 --> 00:00:15,560\"      \"A natural high, I suppose?\"         ##  [9] \"\"                                   \"3\"                                  ## [11] \"00:00:15,640 --> 00:00:17,160\"      \"Gets your heart racing.\" example_text %>% # character vector   cat(sep = \"\\n\") # concatenate the vectors adding a new line (\\n) between each ## 1 ## 00:00:11,160 --> 00:00:13,920 ## - [man] What do you think love is? ## - It'll be like a fairy tale. ##  ## 2 ## 00:00:14,000 --> 00:00:15,560 ## A natural high, I suppose? ##  ## 3 ## 00:00:15,640 --> 00:00:17,160 ## Gets your heart racing. example_text %>% # character vector   str_remove(pattern = \"^\\\\d.*\") %>% # remove non-linguistic lines with regex   cat(sep = \"\\n\") # concatenate vectors ##  ##  ## - [man] What do you think love is? ## - It'll be like a fairy tale. ##  ##  ##  ## A natural high, I suppose? ##  ##  ##  ## Gets your heart racing."},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"reshaping-data","dir":"Articles","previous_headings":"Coding strategies","what":"Reshaping data","title":"7. Tidying data: regular expressions and reshaping datasets","text":"Another key skill acquire proficiency reshaping data. Reshaping refers changing dimensions dataset –column-wise / row-wise –better fit goals researcher.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"columns","dir":"Articles","previous_headings":"Coding strategies > Reshaping data","what":"Columns","title":"7. Tidying data: regular expressions and reshaping datasets","text":"Let’s look column-wise operations first. Consider following single column data frame . example data frame column doc_id three observations. observations doc_id happen contain various pieces information like separate columns namely title TV series, season number, episode number. Let’s shoot get dataset following column structure. separate columns use separate() function tidyr package. takes four main arguments, dataset data, column separate col, new column names separate columns , pattern use separate values columns sep. single column table see period . separates word series title, also separates title season episode group. Let’s apply separate() create five new columns correspond number columns result breaking values .. Since need period ‘character’ operator (.) regular expressions, need escape period \\\\. (Remember R backslashes regular expressions escaped –therefore \\\\. \\.). now five columns, named generically moment. column_5 contains season episode. case clear character use break column two. closer inspection can see can use letter ‘E’ (‘episode’) separate values. mean ‘E’ eliminated, ’s OK column title sufficiently informative. Now opposite separate() unite(). function gather set columns values together. Let’s unite first four columns call series adding whitespace value (sep = \" \"). Just completeness, let’s use newly acquired knowledge regular expressions remove ‘S’ values season column.","code":"example_df <-    tribble(     ~doc_id,     \"Love.On.The.Spectrum.S01E01\",     \"Love.On.The.Spectrum.S01E02\",     \"Love.On.The.Spectrum.S01E03\"   )  example_df # show data frame tribble(   ~series, ~season, ~episode,   \"Love On The Spectrum\", \"01\", \"01\",   \"Love On The Spectrum\", \"01\", \"02\",   \"Love On The Spectrum\", \"01\", \"03\" ) example_df %>% # example dataset   separate(col = doc_id, # column to separate            into = c(\"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\"), # new column names            sep = \"\\\\.\") # expression to separate the columns example_df %>% # example dataset   separate(col = doc_id, # column to separate            into = c(\"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\"), # new column names            sep = \"\\\\.\") %>%  # expression to separate the columns   separate(col = column_5, # column to separate            into = c(\"season\", \"episode\"),  # new column names            sep = \"E\") # expression to separate the columns example_df %>% # example dataset   separate(col = doc_id, # column to separate            into = c(\"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\"), # new column names            sep = \"\\\\.\") %>%  # expression to separate the columns   separate(col = column_5, # column to separate            into = c(\"season\", \"episode\"),  # new column names            sep = \"E\") %>%  # expression to separate the columns   unite(col = \"series\", # new column name         column_1:column_4, # columns to unite (using the between operator `:`)         sep = \" \") # separator to use to join the values of the columns example_df %>% # example dataset   separate(col = doc_id, # column to separate            into = c(\"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\"), # new column names            sep = \"\\\\.\") %>%  # expression to separate the columns   separate(col = column_5, # column to separate            into = c(\"season\", \"episode\"),  # new column names            sep = \"E\") %>%  # expression to separate the columns   unite(col = \"series\", # new column name         column_1:column_4, # columns to unite (using the between operator `:`)         sep = \" \") %>%  # separator to use to join the values of the columns   mutate(season = str_remove(season, pattern = \"^S\")) # remove the S from the values of season"},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"rows","dir":"Articles","previous_headings":"Coding strategies > Reshaping data","what":"Rows","title":"7. Tidying data: regular expressions and reshaping datasets","text":"can imagine, occasions might want change dimensions dataset rows. Much fashion saw previous section worked expanding collapsing columns, can expand collapse rows. typical task perform text analysis linguistic units within column across rows linguistic level. Let’s create example data frame work demonstrate various R programming strategies expanding collapsing rows textual data. example_df three columns. Two columns correspond metadata third text. text includes sentences. observations, stand, correspond student’s comments given class. like observations correspond smaller linguistic unit, say sentences, words, etc. want expand rows desired linguistic unit –still maintaining appropriate metadata information. indispensable package working text data frames tidytext package. workhorse tidytext package unnest_tokens() function. function allows us separate rows column text rows corresponding smaller linguistic unit ‘token’ maintaining tidy format overall dataset. Included unnest_tokens() function option create number common token types (characters, words, ngrams, sentences, etc.) one can specify criterion manually using regular expressions. let’s apply unnest_tokens() example_df data frame. First let’s look breaking comments column sentences. see data frame reshaped rows –importantly maintaining metadata information! way data frame continues conform tidy dataset principles. note fact default unnest_tokens() lowercase text drop original/ input column. can override one settings. want use one default token types, can set token = regex provide new argument pattern =. value pattern argument regular expression. say wanted break comments periods ., question marks exclamation points, way. Note since approach uses regular expression splitting, regex matches deleted output (.e. periods gone). Let’s turn situation data frame sentences observations. imagine want collapse sentences grouping together student_id class_id, effectively re-creating original data frame started . turn tidyverse tools. group_by() specifies column(s) used unite rows. summarize() similar mutate() creates new column. different, however, return grouping column(s) new column created operation contained within summarize(). case applied str_flatten() function, stringr package, unite comment_sentences column values. collapse = argument let’s use specify character(s) used separate united values.","code":"example_df <-    tribble(     ~student_id, ~class_id, ~comments,     \"421\", \"A\", \"This class was fun! I would like to continue to work with my final essay and potentially turn in into an honors thesis. I will contact my professor to find out if this is possibility. Thanks!\",     \"238\", \"B\", \"What shoud I say? I thought I was going to like the class but we did not get to do a final essay. I was hoping to develop a topic in this class that would help me work towards applying for an honors thesis.\"   )  example_df example_df %>%    unnest_tokens(output = \"comment_sentences\", # new column name                 input = \"comments\", # input column                 token = \"sentences\") # token type to create example_df %>%    unnest_tokens(output = \"comment_sentences\", # new column name                 input = \"comments\", # input column                 token = \"sentences\", # token type to create                 to_lower = FALSE, # don't lowercase the output                 drop = FALSE) # don't drop the input column example_df %>%    unnest_tokens(output = \"comment_sentences\", # new column name                 input = \"comments\", # input column                 token = \"regex\", # use a regular expression                 pattern = \"\\\\.\") # specify the regex to use for splitting example_df <-    example_df %>%    unnest_tokens(output = \"comment_sentences\", # new column name                 input = \"comments\", # input column                 token = \"sentences\", # token type to create                 to_lower = FALSE) # don't lowercase the output  example_df example_df %>% # dataset with comment_sentences column   group_by(student_id, class_id) %>% # group by student_id and class_id   summarize(comments = str_flatten(string = comment_sentences, # unite comment_sentences                                    collapse = \" \")) %>%  # join comment_sentences by a single whitespace character   ungroup() # remove grouping attribute on the data frame"},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"case","dir":"Articles","previous_headings":"","what":"Case","title":"7. Tidying data: regular expressions and reshaping datasets","text":"let’s put regular expression reshaping data skills work practical example. case looking curating data TV subtitles Love Spectrum series docuseries following dating experiences individuals Autism spectrum Australia. data manually downloaed Opensubtitles.org files use SRT formatted, seen . format used display subtitles correct moment screen information encodes sequence subtitles show, actual timestamp duration give precise duration subtitle show, text shown. text may conventions display text particular ways / signal cues hearing impaired ([man], example). first need read files R session. found directory structure: read files use readtext() function. main argument file takes path file(s). case ’ve added wildcard * allow reading multiple files .srt extension. (Note: * wildcard regular expression convention function much like regex .*.) output glimpse() see structure lots_files two columns doc_id text three rows.","code":"1 00:00:11,160 --> 00:00:13,920 - [man] What do you think love is? - It'll be like a fairy tale.  2 00:00:14,000 --> 00:00:15,560 A natural high, I suppose?  3 00:00:15,640 --> 00:00:17,160 Gets your heart racing. recipe_7/data/ ├── derived/ └── original/     └── love_on_the_spectrum/         ├── Love.On.The.Spectrum.S01E01.WEBRip.x264-ION10.srt         ├── Love.On.The.Spectrum.S01E02.WEBRip.x264-ION10.srt         └── Love.On.The.Spectrum.S01E03.WEBRip.x264-ION10.srt lots_files <-    readtext(file = \"recipe_7/data/original/love_on_the_spectrum/*.srt\", # only .srt files            verbosity = 0) %>%  # suppress warnings   as_tibble() # convert to a tibble for extended data frame features  glimpse(lots_files) # preview dataset ## Rows: 3 ## Columns: 2 ## $ doc_id <chr> \"Love.On.The.Spectrum.S01E01.WEBRip.x264-ION10.srt\", \"Love.On.T… ## $ text   <chr> \"1\\n00:00:11,160 --> 00:00:13,920\\n- [man] What do you think lo…"},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"curate-metadata","dir":"Articles","previous_headings":"Case","what":"Curate metadata","title":"7. Tidying data: regular expressions and reshaping datasets","text":"Let’s select just doc_id column look values. see names SRT files appears doc_id column. names pattern, period . separating number pieces information –including series name, season, episode. Let’s work separate information distinct columns effectively expanding columns dataset. idealized structure look like example . Let’s use separate() function apply doc_id column. count number columns created breaking values doc_id period . eight columns. want work first five, however. Knowing can set argument character vector names five new columns. sep argument period \\\\. extra columns (remaining three eight) set dropped (.e. included). Now five new columns derived doc_id column. Note names columns arbitrary. Since know changes columns worry creating meaningful column names. name_5 column contans season episode information. want break column two columns pieces information. look values name_5 see way split/ separate values consistent. turns can use ‘E’ episode separate values season episode. new columns season episode appear now. Let’s now create series column uniting first four columns (.e. name_1 name_4). can use unite() function . unite() needs name new column (col), vector columns unite (case name_1:name_4, equivalent c(name_1, name_2, name_3, name_4)), separator new values (sep). last thing clean season column values removing ‘S’ season number. can just use str_remove(). Notice use mutate() function modify columns, case going modify season column going overwrite existing season column results str_remove(). Great! now metadata information curated aimed .","code":"lots_files %>%     select(doc_id)  # only show doc_id lots_files_separate <-    lots_files %>% # dataset   separate(col = doc_id, # column to separate            into = c(\"name_1\", \"name_2\", \"name_3\", \"name_4\", \"name_5\"), # new column names            sep = \"\\\\.\", # pattern to separate the column values            extra = \"drop\") # drop any extra columns created in the separation  lots_files_separate %>% # separated dataset   select(-text) # do not show the `text` column here lots_files_separate <-    lots_files_separate  %>% # dataset   separate(col = name_5, # column to separate            into = c(\"season\", \"episode\"), # new columns            sep = \"E\") # pattern to use to separate the values  lots_files_separate %>% # separated dataset   select(-text) # do not show the `text` column here lots_files_unite <-    lots_files_separate %>% # dataset   unite(col = \"series\", # new column name         name_1:name_4, # columns to unite         sep = \" \") # separator to add between of the column values  lots_files_unite %>% # united dataset   select(-text) # do not show the `text` column here lots_meta <-    lots_files_unite %>% # dataset   mutate(season = str_remove(season, \"S\")) # remove 'S' in season and overwrite existing season column  glimpse(lots_meta) # preview full dataset ## Rows: 3 ## Columns: 4 ## $ series  <chr> \"Love On The Spectrum\", \"Love On The Spectrum\", \"Love On The S… ## $ season  <chr> \"01\", \"01\", \"01\" ## $ episode <chr> \"01\", \"02\", \"03\" ## $ text    <chr> \"1\\n00:00:11,160 --> 00:00:13,920\\n- [man] What do you think l…"},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"curate-text","dir":"Articles","previous_headings":"Case","what":"Curate text","title":"7. Tidying data: regular expressions and reshaping datasets","text":"’s now time turn attention working text column. Let’s take look first 500 characters first observation text column. see output row text column dialogue series, season, episode row. also see dialogue conducive language research given SRT formatting. Ideally like clean SRT information (sequence, timestamps, non-linguistic cues) retain dialogue. idealized dataset data look something like : , get started think go getting idealized format given format text R programming strategies disposal. First, always good idea look closely internal structure text. Looking text can see observation text series lines, separated \\n. part distinct lines contain either dialogue SRT conventions displaying subtitles screen. potential first step cleaning data split observation lines. can using unnest_tokens() function use regular expression separate new line anchors \\\\n. can see first 10 observations consistent pattern lines start digit dialogue. ’s worthwhile open one original .srt files text editor (RStudio text editing software) ensure case. inspection look like safe target lines starting digits remove. Just sanity check, however, can instead search lines start digits make sure get something like keep. search lines start digits, can craft regular expression use str_detect() function. return logical value, TRUE ’s match, FALSE otherwise. Wrapping str_detect() filter() filter rows return TRUE. Looks like safe move. Let’s go ahead remove lines. use str_remove() require match entire pattern (line) want remove –can just leverage search just reverse logical value FALSE matches. Remember can adding ! operator front str_detect() function returns logical value. Great. Let’s add code continue clean text. next aspect want get rid hyphens starting lines. Let’s get rid . need use str_remove() just removing portion line, namely pattern captured ^-\\\\s (hyphens beginning line followed single whitespace). Great. ‘-’ leading line now gone. Let’s now explore ’s going lines start [ appear include cues speaking. cases see first 10 observations see [man], let’s search lines starting [ find examples cue text. Since [ character means something regular expressions need escape . use expression ^\\\\[. Running code couple times R Markdown, get different samples lines starting [. results seems like brackets used beginning lines signal types cues. want remove –don’t want remove proper names. names capital letter just inside [. Let’s use regular expression filter lines start [ followed lowercase letter. regular expression ^\\\\[[-z]. Now removed lines clearly marked cues dialogue speakers interested , can now remove cases text inside brackets, [David laughs] example. Let’s try search extract pattern. isolate pattern return match use str_extract() function. match regular expression pattern return complete match. able evaluate whether pattern working use mutate() assign matches new column variable easier comparison original text lines column. Let’s start pattern \\\\[.+\\\\] \\\\[.+\\\\] regex appears want, now let’s apply remove cues. case use str_remove() removing characters within line, entire line. since may one instance pattern single line, use str_remove_all() function. text lines looking pretty clean. Let’s check see patterns might signal artifacts might want eliminate dialogue. Let’s see lines start non-alphabetic non-digit characters. can accomplish ^\\\\W expression. Running code times get sample ^\\\\W pattern matches get results clearly artifacts (‘<>’) ‘Yes.’. let’s play safe filter non-word character, instead remove lines start appear HTML tags ‘<tag>’. regular expression ’ve worked ^<.+?>.*. Alright. text almost clean ’s going get. notice inspection ‘<>…<\/>’ tags around words, words used target speakers. let’s keep text inside remove leading trailing tag expression <.+?>. Now can assign work done step--step clean data new data frame lots_meta_lines_clean. final step curate data structure idealized collapse lines column dialogue grouping series, season, episode. ’s dataset looks now, dialogue truncated 200 characters display .","code":"lots_meta$text[1] %>% # subset the first observation of the text column   str_trunc(width = 500) %>% # truncate the values to 500 characters   cat() # print the characters for inspection 1 00:00:11,160 --> 00:00:13,920 - [man] What do you think love is? - It'll be like a fairy tale.  2 00:00:14,000 --> 00:00:15,560 A natural high, I suppose?  3 00:00:15,640 --> 00:00:17,160 Gets your heart racing.  4 00:00:17,240 --> 00:00:19,600 Can make people do crazy things.  5 00:00:19,680 --> 00:00:23,320 You just feel very warm inside yourself as a person.  6 00:00:23,400 --> 00:00:25,360 - [man] So, have you ever been in love? - No.  7 00:00:25,440 --> 00:00:27,240 - [man] Have you be... lots_files$text[1] %>% # first observation of the `text` column   str_trunc(width = 200) # only show the first 200 characters ## [1] \"1\\n00:00:11,160 --> 00:00:13,920\\n- [man] What do you think love is?\\n- It'll be like a fairy tale.\\n\\n2\\n00:00:14,000 --> 00:00:15,560\\nA natural high, I suppose?\\n\\n3\\n00:00:15,640 --> 00:00:17,160\\nGets yo...\" lots_meta_lines <-    lots_meta %>% # dataset with `text` column   unnest_tokens(output = lines, # new column                 input = text, # input column                 token = \"regex\", # use regular expression to split                 pattern = \"\\\\n\", # split by new line anchor                 to_lower = FALSE) # do not lowercase the text  lots_meta_lines %>% # dataset   slice_head(n = 10) # preview first 10 observations lots_meta_lines %>% # dataset   filter(str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with  a digit   slice_sample(n = 10) # a random sample 10 observations from the entire dataset lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   slice_head(n = 10) # first 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   slice_head(n = 10) # first 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(str_detect(lines, \"^\\\\[\")) %>% # find lines starting with `[`   slice_sample(n = 10) # random sample of 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   slice_sample(n = 10) # random sample of 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   mutate(match = str_extract(lines, \"\\\\[.+\\\\]\")) %>% # search for cues inside of brackets ex. [man]   slice_sample(n = 10) # random sample of 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   mutate(lines = str_remove_all(lines, \"\\\\[.+\\\\]\")) %>% # remove cues inside of brackets ex. [man]   slice_sample(n = 10) # random sample of 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   mutate(lines = str_remove_all(lines, \"\\\\[.+\\\\]\")) %>% # remove cues inside of brackets ex. [man]   filter(str_detect(lines, \"^\\\\W\")) %>% # search lines starting with non-word character   slice_sample(n = 10) # random sample of 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   mutate(lines = str_remove_all(lines, \"\\\\[.+\\\\]\")) %>% # remove cues inside of brackets ex. [man]   filter(!str_detect(lines, \"^<.+?>.*\")) %>% # remove lines starting with HTML tags   slice_sample(n = 10) # random sample of 10 observations lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   mutate(lines = str_remove_all(lines, \"\\\\[.+\\\\]\")) %>% # remove cues inside of brackets ex. [man]   filter(!str_detect(lines, \"^<.+?>.*\")) %>% # remove lines starting with HTML tags   mutate(lines = str_remove_all(lines, \"<.+?>\")) %>%  # remove all HTML tags   slice_sample(n = 10) # random sample of 10 observations lots_meta_lines_clean <-    lots_meta_lines %>% # dataset   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   mutate(lines = str_remove_all(lines, \"\\\\[.+\\\\]\")) %>% # remove cues inside of brackets ex. [man]   filter(!str_detect(lines, \"^<.+?>.*\")) %>% # remove lines starting with HTML tags   mutate(lines = str_remove_all(lines, \"<.+?>\"))  # remove all HTML tags lots_curated <-    lots_meta_lines_clean %>% # dataset   group_by(series, season, episode) %>% # grouping   summarise(dialogue = str_flatten(lines, collapse = \" \")) %>% # collapse lines single observations for each series, season, episode combination   ungroup() %>% # remove grouping attributes   mutate(dialogue = str_trim(dialogue, side = \"both\")) # trim any leading or trailing whitespace from the united/ flattened lines in dialogue lots_curated %>% # curated dataset   mutate(dialogue = str_trunc(dialogue, 200)) # just show first 200 characters of dialogue"},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"write-and-document","dir":"Articles","previous_headings":"Case","what":"Write and document","title":"7. Tidying data: regular expressions and reshaping datasets","text":"last things write curated dataset lots_curated disk create data dictionary. ’m going add function described Text Data coursebook. Now ’m going call data_dic_starter() lots_curated dataset open file spreadsheet software add human-readable variable names descriptions file. summarize quite long case, ’ve included concise form code curate Love Spectrum SRT files. Wow! quite journey.","code":"fs::dir_create(path = \"recipe_7/data/derived/love_on_the_spectrum/\") write_csv(lots_curated, # curated dataset           file = \"recipe_7/data/derived/love_on_the_spectrum/lots_curated.csv\") data_dic_starter <- function(data, file_path) {   # Function:   # Creates a .csv file with the basic information   # to document a curated dataset      tibble(variable_name = names(data), # column with existing variable names         name = \"\", # column for human-readable names        description = \"\") %>% # column for prose description   write_csv(file = file_path) # write to disk } data_dic_starter(data = lots_curated, # curated dataset                  file_path = \"recipe_7/data/derived/love_on_the_spectrum/lots_curated_data_dictionary.csv\") recipe_7/data/ ├── derived/ │   └── love_on_the_spectrum/ │       ├── lots_curated.csv │       └── lots_curated_data_dictionary.csv └── original/     └── love_on_the_spectrum/         ├── Love.On.The.Spectrum.S01E01.WEBRip.x264-ION10.srt         ├── Love.On.The.Spectrum.S01E02.WEBRip.x264-ION10.srt         └── Love.On.The.Spectrum.S01E03.WEBRip.x264-ION10.srt # Read the SRT files lots_files <-    readtext(file = \"recipe_7/data/original/love_on_the_spectrum/*.srt\", # only .srt files            verbosity = 0) %>%  # suppress warnings   as_tibble() # convert to a tibble for extended data frame features  # Curate metadata lots_meta <-    lots_files %>% # dataset   separate(col = doc_id, # column to separate            into = c(\"name_1\", \"name_2\", \"name_3\", \"name_4\", \"name_5\"), # new column names            sep = \"\\\\.\", # pattern to separate the column values            extra = \"drop\") %>%  # drop any extra columns created in the separation   separate(col = name_5, # column to separate            into = c(\"season\", \"episode\"), # new columns            sep = \"E\") %>%  # pattern to use to separate the values   unite(col = \"series\", # new column name         name_1:name_4, # columns to unite         sep = \" \") %>%  # separator to add between of the column values   mutate(season = str_remove(season, \"S\")) # remove 'S' in season and overwrite existing season column  # Curate text # Clean text by lines lots_meta_lines <-    lots_meta %>% # dataset with `text` column   unnest_tokens(output = lines, # new column                 input = text, # input column                 token = \"regex\", # use regular expression to split                 pattern = \"\\\\n\", # split by new line anchor                 to_lower = FALSE) %>% # do not lowercase the text   filter(!str_detect(lines, \"^\\\\d\")) %>% # detect lines starting with a digit and remove them   mutate(lines = str_remove(lines, \"^-\\\\s\")) %>% # remove leading '- ' from each line   filter(!str_detect(lines, \"^\\\\[[a-z]\")) %>% # remove lines starting with `[` before a lowercase letter   mutate(lines = str_remove_all(lines, \"\\\\[.+\\\\]\")) %>% # remove cues inside of brackets ex. [man]   filter(!str_detect(lines, \"^<.+?>.*\")) %>% # remove lines starting with HTML tags   mutate(lines = str_remove_all(lines, \"<.+?>\"))  # remove all HTML tags  # Collapse lines by series, season, and episode lots_curated <-    lots_meta_lines %>%    group_by(series, season, episode) %>% # grouping   summarise(dialogue = str_flatten(lines, collapse = \" \")) %>% # collapse lines single observations for each series, season, episode combination   ungroup() %>% # remove grouping attributes   mutate(dialogue = str_trim(dialogue, side = \"both\")) # trim any leading or trailing whitespace from the united/ flattened lines in dialogue  # Write the dataset to disk fs::dir_create(path = \"recipe_7/data/derived/love_on_the_spectrum/\") write_csv(lots_curated, # curated dataset           file = \"recipe_7/data/derived/love_on_the_spectrum/lots_curated.csv\")  # Create data dictionary to open and edit data_dic_starter <- function(data, file_path) {   # Function:   # Creates a .csv file with the basic information   # to document a curated dataset      tibble(variable_name = names(data), # column with existing variable names         name = \"\", # column for human-readable names        description = \"\") %>% # column for prose description   write_csv(file = file_path) # write to disk }  data_dic_starter(data = lots_curated,                   file_path = \"recipe_7/data/derived/love_on_the_spectrum/lots_curated_data_dictionary.csv\")"},{"path":"https://lin380.github.io/tadr/articles/recipe_7.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"7. Tidying data: regular expressions and reshaping datasets","text":"Recipe , , covered lot ground. Key programming strategies included regular expressions reshaping data. process reviewed extended use many functions tidyverse meta-package also included powerful function unnest_tokens() tidytext package. continue work principles packages next recipe move refine curated dataset align directly analysis method.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"8. Dataset manipulation: tokenization and joining datasets","text":"Recipe look two primary types transformations, tokenization joins. Tokenization process recasting textual units smaller textual units. process joining datasets aims incorporate datasets augment filter dataset interest. first look sample dataset explore strategies associated tokenization joins put practice practical example. Let’s load packages use Recipe.","code":"library(tidyverse, quietly = TRUE)  # data manipulation library(tidytext)  # tokenization"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"coding-strategies","dir":"Articles","previous_headings":"","what":"Coding strategies","title":"8. Dataset manipulation: tokenization and joining datasets","text":"illustrate relevant coding strategies ’ve created curated dataset “Big Data Set RateMyProfessor.com Professors’ Teaching Evaluation” (2020). Let’s take look curated dataset get oriented structure. see 10 observations four columns. data dictionary associated rmp curated dataset. Let’s read show human-readable format. Table 1: Rate Professor curated sample data dictionary. Now let’s look small curated sample current form. Table 2: Rate Professor curated sample preview. orientation dataset can see four columns id, online, student_star, comments. first three metadata associated text comments. also see online column contains five positive comments five negative comments.","code":"rmp <- read_csv(file = \"recipe_8/data/derived/rate_my_professor_sample/rmp_curated.csv\")  # read curated dataset  glimpse(rmp)  # preview structure ## Rows: 10 ## Columns: 4 ## $ rating_id    <dbl> 89, 91, 5770, 3350, 1763, 4918, 2462, 982, 4734, 7903 ## $ online       <dbl> 0, 0, 0, 0, 0, 1, 1, 1, 1, 1 ## $ student_star <dbl> 1, 5, 5, 5, 5, 5, 5, 1, 5, 1 ## $ comments     <chr> \"By far the most condescending, mean, disrespectful, and … read_csv(file = \"recipe_8/data/derived/rate_my_professor_sample/rmp_curated_data_dictionary.csv\") %>% # read data dictionary   knitr::kable(booktabs = TRUE,                caption = \"Rate My Professor curated sample data dictionary.\") # show preview table rmp %>% # dataset   knitr::kable(booktabs = TRUE,                caption = \"Rate My Professor curated sample preview.\") # show dataset preview"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"tokenization","dir":"Articles","previous_headings":"Coding strategies","what":"Tokenization","title":"8. Dataset manipulation: tokenization and joining datasets","text":"helpful function unnest_tokens() tidytext package efficient way recast column text various smaller textual units –maintaining metadata structure curated dataset. way, transformation maintain tidy data format. Let’s consider key options tokenization provided unnest_tokens() function. First let’s look arguments using args() function. order appearance function, tbl argument takes data frame, output character vector desired name output column tokenization, input character vector names column contains textual information tokenized, token argument specify type token like generate input column, format argument often left default ‘text’ –often working text, drop argument default drop input column tokenized dataset, to_lower argument let’s us decide want lowercase text tokenized, collapse argument allows grouping tokenization output often left ‘NULL’ (default), finally ... argument leaves possibility adding arguments relevant token options, specifically ‘ngrams’ ‘character_shingles’. Let’s see unnest_tokens() action starting first common tokenization unit (therefore default) ‘words’.","code":"args(unnest_tokens)  # view the arguments ## function (tbl, output, input, token = \"words\", format = c(\"text\",  ##     \"man\", \"latex\", \"html\", \"xml\"), to_lower = TRUE, drop = TRUE,  ##     collapse = NULL, ...)  ## NULL"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"words","dir":"Articles","previous_headings":"Coding strategies > Tokenization","what":"Words","title":"8. Dataset manipulation: tokenization and joining datasets","text":"now see preview first 10 observations words comments tokenized. unnest_tokens() return tokens row maintain metadata original dataset (dropping input comments column). also see default tokens lowercased, , default behavior. Let’s change drop = argument to_lower = argument defaults (TRUE). Note textual input punctuation, unnest_tokens() function strip punctuation tokenization words.","code":"rmp %>% # dataset   unnest_tokens(output = \"word\", # tokenized output column                 input = \"comments\") %>% # input column to tokenize   slice_head(n = 10) # preview first 10 observations rmp %>% # dataset   unnest_tokens(output = \"word\", # tokenized output column                 input = \"comments\", # input column to tokenize                 to_lower = FALSE, # do not lowercase                 drop = FALSE) %>%  # do not drop input column   slice_head(n = 10) # preview first 10 observations"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"sentences","dir":"Articles","previous_headings":"Coding strategies > Tokenization","what":"Sentences","title":"8. Dataset manipulation: tokenization and joining datasets","text":"specify tokenized unit sentences, punctuation stripped. take close look output using sentence tokens case see multiple sentences observation row. appears due fact students sometimes opted capitalize beginning next sentence. suggests algorithm unnest_tokens() uses sentences punctuation followed capitalized word segment/ tokenize sentences. important review output tokenization catch types anomalies assume algorithm perfectly accurate. tokenization defaults (words, sentences, etc.) produce desired result, can specify token = argument regex. allows us specify regular expression pattern tokenization added argument pattern =. Note pattern used segment text matched, match removed. can use regular expression magic ‘positive lookbehind’ operator (?<=) detect pattern, use match. case apply punctuation part original regex, can preserve sentence punctuation still segment sentences.","code":"rmp %>% # dataset   unnest_tokens(output = \"sentence\", # tokenized output column                 input = \"comments\", # input column to tokenize                 token = \"sentences\", # tokenize to sentences                 to_lower = FALSE, # do not lowercase                 drop = FALSE) %>%  # do not drop input column   slice_head(n = 10) # preview first 10 observations rmp %>% # dataset   unnest_tokens(output = \"sentence\", # tokenized output column                 input = \"comments\", # input column to tokenize                 token = \"regex\", # tokenize by a regex pattern                 pattern = \"[.!?]\\\\s\",                 to_lower = FALSE, # do not lowercase                 drop = FALSE) %>%  # do not drop input column   slice_head(n = 10) # preview first 10 observations rmp %>% # dataset   unnest_tokens(output = \"sentence\", # tokenized output column                 input = \"comments\", # input column to tokenize                 token = \"regex\", # tokenize by a regex pattern                 pattern = \"(?<=[.!?])\\\\s\",                 to_lower = FALSE, # do not lowercase                 drop = FALSE) %>%  # do not drop input column   slice_head(n = 10) # preview first 10 observations"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"ngrams","dir":"Articles","previous_headings":"Coding strategies > Tokenization","what":"Ngrams","title":"8. Dataset manipulation: tokenization and joining datasets","text":"Now let’s turn ngram tokenization. ngram sequence words \\(n\\) sequence desired output. Word tokenization sometimes called unigram. get ngrams larger one word, use specify token = ngrams. need add argument n = set number word sequences want tokenize. n = 2 produce bigrams, n = 3 trigrams, . let’s see action creating bigrams. Great. now two word sequences (bigrams) tokens. look ouput see tokenization bigrams included sequences span sentences (ex. ‘teacher wouldnt’. due fact used original input (comments) text. cases may want capture cross-sentential word sequences. avoid can first tokenize comments sentences (regular expression approach), pass result bigram tokenization. applying firs sentence tokenization ngram tokenization avoid cross-sentential word sequences. Note added sentence_id column make sure sentence bigram comes documented dataset. overview options strategies tokenizing textual input, now create word-based tokenization rmp dataset, lowercasing text preparation next strategy cover, joins. Table 3: Preview rmp_words dataset.","code":"rmp %>% # dataset   unnest_tokens(output = \"bigram\", # tokenized output column                 input = \"comments\", # input column to tokenize                 token = \"ngrams\", # tokenize ngram sequences                 n = 2, # two word sequences                 to_lower = FALSE, # do not lowercase                 drop = FALSE) %>%  # do not drop input column   slice_head(n = 10) # preview first 10 observations rmp %>% # dataset   # Tokenize by sentences   unnest_tokens(output = \"sentence\", # tokenized output column                 input = \"comments\", # input column to tokenize                 token = \"regex\", # tokenize by a regex pattern                 pattern = \"(?<=[.!?])\\\\s\",                 to_lower = FALSE) %>%  # do not lowercase   # Add a sentence_id to the dataset   group_by(rating_id) %>% # group the comments   mutate(sentence_id = row_number()) %>% # add a sentence id to index the individual sentences for each comment    ungroup() %>% # remove grouping attribute   # Tokenize the sentences by bigrams   unnest_tokens(output = \"bigram\", # tokenized output column                 input = \"sentence\", # input column to tokenize                 token = \"ngrams\", # tokenize by ngrams                 n = 2, # create bigrams                 to_lower = FALSE) %>%  # do not lowercase   slice_head(n = 10) # preview first 10 observations rmp_words <-    rmp %>% # dataset   unnest_tokens(output = \"word\", # tokenized output column                 input = \"comments\") # input column to tokenize  rmp_words %>%    slice_head(n = 10) %>%    knitr::kable(booktabs = TRUE,                 caption = \"Preview of the `rmp_words` dataset.\")"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"joining-datasets","dir":"Articles","previous_headings":"Coding strategies","what":"Joining datasets","title":"8. Dataset manipulation: tokenization and joining datasets","text":"dplyr package, loaded part tidyverse, contains number functions aimed joining datasets. functions two main types: mutating joins filtering joins. cases join relates two datasets share column (column) overlapping values. mutating joins, shared column/s /key connects two datasets effectively expands columns combining columns dataset values match across datasets. filtering joins, shared column effectively used filter rows dataset matching values datasets. Filter may used exclude matching values, include values match. Let’s look two types joins get better sense behavior.","code":""},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"mutating-joins","dir":"Articles","previous_headings":"Coding strategies > Joining datasets","what":"Mutating joins","title":"8. Dataset manipulation: tokenization and joining datasets","text":"demonstration, let’s consider dataset included tidytext package provides list words sentiment value word. can see get_sentiments() function returns dataset two columns (word sentiment). ’ve provided first five word-sentiment pairs ‘negative’ ‘positive’ sentiments. However, full dataset contains 6786 words. can see many listed positive negative. can see negative words outnumber positive-labeled words. information, can now see rmp_words dataset dataset get_sentiments() share column called word. importantly, columns share type values, .e. words. wanted augment rmp_words dataset sentiment labels get_sentiments() want use mutating join. idea create data frame following structure: structure want observations (words) rmp_words appear words matches get_sentiments() also get corresponding sentiment value. use left_join() function. function takes primary arguments x y x dataset want observations included y matching values also get corresponding values. Note left_join() keeps rows x dataset –case rmp_words. , example, wanted mutating join remove words x match y, can turn inner_join(). inner_join() essence mutating join filtering side effect. want simply filter dataset based values dataset, turn filtering joins.","code":"get_sentiments() %>%     group_by(sentiment) %>%     slice_head(n = 5) get_sentiments() %>%     count(sentiment) tribble(   ~rating_id, ~online, ~student_star, ~word, ~sentiment,   84, 0, 5, \"good\", \"positive\",   84, 0, 5, \"teacher\", NA,   2802, 1, 1, \"worst\", \"negative\",   NA, NA, NA, \"...\", \"...\" ) left_join(rmp_words, get_sentiments()) %>%     slice_head(n = 10) inner_join(rmp_words, get_sentiments()) %>%     slice_head(n = 10)"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"filtering-joins","dir":"Articles","previous_headings":"Coding strategies > Joining datasets","what":"Filtering joins","title":"8. Dataset manipulation: tokenization and joining datasets","text":"look filtering joins let’s consider another dataset also included tidytext package get_stopwords(). Stopwords words considered little semantic content (roughly correspond pronouns, prepositions, conjunctions, etc.). research cases want remove words dataset. remove words can use filtering join called anti_join(), can imagine return rows x match y. see now stopwords removed rmp_words dataset. Now want inverse operation, keeping stopwords rmp_words can use semi_join() function. One last case worth including filtering join takes character vector, data frame. %% operator can used semi_join() keeping matching values x anti_join() removing values x. Note filtering joins, new columns added, rows affected.","code":"get_stopwords() %>%     slice_head(n = 10) anti_join(rmp_words, get_stopwords()) %>%     slice_head(n = 10) semi_join(rmp_words, get_stopwords()) %>%     slice_head(n = 10) rmp_words %>%    filter(word %in% c(\"very\", \"teacher\")) %>%  # keep matching rows   slice_head(n = 10) rmp_words %>%    filter(!word %in% c(\"very\", \"teacher\")) %>%  # remove matching rows   slice_head(n = 10)"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"case","dir":"Articles","previous_headings":"","what":"Case","title":"8. Dataset manipulation: tokenization and joining datasets","text":"Let’s now turn practical case see tokenization joins action. work Love Spectrum curated dataset previously worked . aim tokenize dataset words join imported dataset contains word frequencies calculated corpus TV/Film transcripts, SUBTLEXus word frequencies. ’ll read dataset clean columns relevant columns transformational goals. result transformation aims produce following dataset structure:","code":"# Read the curated dataset for Love on the Spectrum Season 1 lots <- read_csv(file = \"recipe_8/data/derived/love_on_the_spectrum/lots_curated.csv\")  glimpse(lots) ## Rows: 3 ## Columns: 4 ## $ series   <chr> \"Love On The Spectrum\", \"Love On The Spectrum\", \"Love On The … ## $ season   <chr> \"01\", \"01\", \"01\" ## $ episode  <chr> \"01\", \"02\", \"03\" ## $ dialogue <chr> \"It'll be like a fairy tale. A natural high, I suppose? Gets … word_frequencies <-    read_tsv(file = \"recipe_8/data/original/word_frequency_list/SUBTLEXus.tsv\")  word_frequencies <-    word_frequencies %>% # dataset   select(word, word_freq = SUBTLWF) # select columns  word_frequencies %>%    slice_head(n = 10) tribble(   ~series, ~season, ~episode, ~word, ~word_freq,   \"Love On The Spectrum\", \"01\", \"01\", \"it\", 18896 )"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"tokenize","dir":"Articles","previous_headings":"Case","what":"Tokenize","title":"8. Dataset manipulation: tokenization and joining datasets","text":"first step tokenize lots curated dataset word tokens. One thing notice preview words like “’ll” considered one token, two (.e. ‘’ ‘ll’). Let’s use %% filter (.e. search) word_frequencies dataset see words like “’ll” listed. appears ‘’ ‘ll’ treated separate words. Therefore want make sure tokenization lots dataset reflects . original tokenization using default token = \"words\" let’s create regular expression . works, side effect –namely punctuation stripped. get rid punctuation can normalize word column, removing punctuation. appears look good. Let’s now assign ouput object can move join dataset word_frequencies dataset.","code":"# Tokenize dialogue into words lots_words <-    lots %>% # dataset   unnest_tokens(output = \"word\", # output column                 input = \"dialogue\", # input column                 token = \"words\") # tokenized unit  lots_words %>%    slice_head(n = 10) word_frequencies %>% # dataset   filter(word %in% c(\"it'll\", \"it\", \"ll\")) #search for it'll, it, and ll lots %>% # dataset   unnest_tokens(output = \"word\", # output column                 input = \"dialogue\", # input column                 token = \"regex\", # regex tokenization                 pattern = \"(\\\\s|')\") %>% # regex pattern   slice_head(n = 10) # preview lots %>% # dataset   unnest_tokens(output = \"word\", # output column                 input = \"dialogue\", # input column                 token = \"regex\", # regex tokenization                 pattern = \"(\\\\s|')\") %>% # regex pattern   mutate(word = str_remove(word, pattern = \"[:punct:]\")) %>% # remove punctuation   slice_head(n = 10) # preview lots_words <-    lots %>% # dataset   unnest_tokens(output = \"word\", # output column                 input = \"dialogue\", # input column                 token = \"regex\", # regex tokenization                 pattern = \"(\\\\s|')\") %>% # regex pattern   mutate(word = str_remove(word, pattern = \"[:punct:]\")) # remove punctuation"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"join","dir":"Articles","previous_headings":"Case","what":"Join","title":"8. Dataset manipulation: tokenization and joining datasets","text":"Now time join lots_words word_frequencies keeping observations x adding word_freq column words match x y. turn function left_join(). looks good let’s assign operation new object lots_words_freq.","code":"left_join(lots_words, word_frequencies) %>%     slice_head(n = 10) lots_words_freq <- left_join(lots_words, word_frequencies)"},{"path":"https://lin380.github.io/tadr/articles/recipe_8.html","id":"document","dir":"Articles","previous_headings":"Case","what":"Document","title":"8. Dataset manipulation: tokenization and joining datasets","text":"final step process write transformed dataset disk document data dictionary. Using data_dic_starter() can create data dictionary template can open spreadsheet document.","code":"write_csv(lots_words_freq, file = \"recipe_8/data/derived/love_on_the_spectrum/lots_words_freq.csv\") data_dic_starter(data = lots_words_freq, file_path = \"recipe_8/data/derived/love_on_the_spectrum/lots_words_freq_data_dictionary.csv\")"},{"path":[]},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"Recipe work dative dataset languageR package illustrate common coding strategies preparing datasets inferential data analysis, well steps conduct descriptive assessment, statistical interrogation, evaluation reporting results. seen Chapter 8 “Inference” Text Data Coursebook, dative dataset concerns use Dative Alternation. demonstration posit research hypothesis role (either ‘recipient’ ‘theme’) longer phrase ordered last dative construction. related syntactic phenomenon “Heavy NP” shift. illustrate, predicts example (2) . B. preferred (1). . Double Object Dative give [man NP] drug test. [length theme = 3] give [man NP] drug test surely come back negative. [length theme = 9] B. Prepositional Dative give drug test [man PP]. [length recipient = 2] give drug test [man just walked PP]. [length recipient = 6] set dataset format demonstrate strategies, load dataset select subset key variables, making sure vectors encoded factors changed character vectors. important note since dataset included part R package dataset dative stored .rds file. types files allow storing vectors factors. Plain-text files .csv files, however, store data factors. illustrate common scenario reproducible research projects data datasets stored plain-text files, ’ve removed factors. Let’s load package use Recipe now. ’ve also included custom function print_pretty_tables() make printing tables captions little simpler.","code":"library(tidyverse)  # data manipulation library(janitor)    # for `clean_names()` function library(skimr)      # descriptive summaries library(effectsize) # calculate confidence and generate effect size library(report)     # create boilerplate statistical reporting library(patchwork)  # organize plots library(knitr)      # pretty formatted tables  print_pretty_table <- function(data, caption, n = 10) {   # Function   # Print data frames as pretty tables with captions      data %>% # dataset     slice_head(n = n) %>% # first n observations     knitr::kable(booktabs = TRUE, # bold headers                  caption = caption) # caption }"},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"coding-strategies","dir":"Articles","previous_headings":"","what":"Coding strategies","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"Let’s take look structure demo dataset dative. 3,263 observations 4 variables.","code":"dative <- languageR::dative %>%     select(RealizationOfRecipient, Modality, LengthOfRecipient, LengthOfTheme) %>%     mutate_if(is.factor, as.character)  glimpse(dative) ## Rows: 3,263 ## Columns: 4 ## $ RealizationOfRecipient <chr> \"NP\", \"NP\", \"NP\", \"NP\", \"NP\", \"NP\", \"NP\", \"NP\",… ## $ Modality               <chr> \"written\", \"written\", \"written\", \"written\", \"wr… ## $ LengthOfRecipient      <int> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1,… ## $ LengthOfTheme          <int> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8,…"},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"preparation","dir":"Articles","previous_headings":"Coding strategies","what":"Preparation","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"Although process dataset transformation ideally prepare dataset analysis, , times, adjustments need still need take place. may structural nature, dataset requires row-wise (.e. change unit observation) / column-wise (.e. addition removal columns). also may changes less structural recoding character vectors factors even aesthetic (.e. renaming columns renaming values). Working dative dataset, let’s first rename columns follow ‘snake case’ convention ’ve used throughout course. janitor package contains function called clean_names(). function default change column names snake case. two reasons change variable names: 1) find snake case legible 2) want consistent use naming formatting conventions. second point important help avoid typos confusion switching conventions. Table 1: First 10 observations datives dataset snake case column names. Now let’s get know data little better. ’ve provided data dictionary based R Documentation dataset languageR package. Table 2: Data dictionary dative dataset. Now, next step need conduct prepare dataset analysis change character vectors factors. done using factor() function. statistical procedures R generate errors character vectors converted factors. factor vector? Essentially character vector additional attributes allow encoding explicit ordering levels (distinct values variable) / addition labels (may differ actual values). dative dataset two character vectors: realization_of_recipient modality. Now neither character vectors contain information logically ordered, apply level ordering variables. can quickly change character vectors factors using special case mutate() called mutate_if() takes name vector type choose function apply () vectors. default factor() order levels alpha-numerically. Let’s test creating temporary object called dative_fct. can use levels() function see levels factor see ordered. Let’s look modality factor. want order fashion / want change labels levels, can mutating variable applying levels = / labels = arguments character vector values order want ordered levels labels want. case level ordering need change important factors, need make sure label values default ordering label ordering correspond. make sure don’t confuse default ordering addition labels use arguments. Let’s change labels ‘written’ “Written” ‘spoken’ “Spoken” making sure apply labels correct values. now ready move analyze dataset!","code":"dative <-    dative %>% # dataset   clean_names() # convert the column names to snake case  dative %>% # dataset   print_pretty_table(caption = \"First 10 observations of the `datives` dataset with snake case column names.\") dative_fct <- dative %>%     mutate_if(is.character, factor)  glimpse(dative_fct) ## Rows: 3,263 ## Columns: 4 ## $ realization_of_recipient <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, N… ## $ modality                 <fct> written, written, written, written, written, … ## $ length_of_recipient      <int> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, … ## $ length_of_theme          <int> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, … dative_fct$modality %>% # modality column   levels() # view the levels and their order ## [1] \"spoken\"  \"written\" dative <-    dative %>% # dataset   mutate(realization_of_recipient = factor(realization_of_recipient)) %>% # change to factor   mutate(modality = factor(modality,                             levels = c(\"spoken\", \"written\"),                             labels = c(\"Spoken\", \"Written\"))) # change to factor with new level labels  glimpse(dative) ## Rows: 3,263 ## Columns: 4 ## $ realization_of_recipient <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, N… ## $ modality                 <fct> Written, Written, Written, Written, Written, … ## $ length_of_recipient      <int> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, … ## $ length_of_theme          <int> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, …"},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"descriptive-assessment","dir":"Articles","previous_headings":"Coding strategies","what":"Descriptive assessment","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"descriptive assessment going want take look numeric summaries generate relevant plots explore distribution variables used statistical model. get numerical summaries can use function skim() skimr() package. Table 3: Data summary Variable type: factor Variable type: numeric can see skim() function outputs quite bit information dataset. information ‘Data Summary’, however, information already gain use glimpse() function often needed. can use yank() function select variable type like look –either ‘factor’ ‘numeric’. let’s see ‘yank’ factors realization_of_recipient modality first yank(\"factor\") skim(). Variable type: factor Great. Now want get just ‘numeric’ variables, can just run yank(\"numeric\") basic approach. , however, useful way modify information skim() returns. can particularly useful numeric variables tha can get IQR score. can create ‘skim’ object skim_with() function. can define new summary statistics return within sfl() (‘skim function list’) function. add IQR() function R column name irq. name custom skim function num_skim() applicable numeric variables. Variable type: numeric can also use skim() custom skim num_skim() group data frame. effect creating aggregated numeric summaries. Variable type: numeric Since quite variables, output can little hard visually parse. couple things note. First, can see general median aggregated summaries smaller mean –means right skew. Second, look means medians can see central tendency measures suggest length recipient/ theme longer spoken written modalities recipient last (.e. realization recipient ‘PP’) theme last (‘NP’). Let’s visualize metrics. Given two categorical variables (one dependent) two continuous variables, choose box plot. twist need create two box plots one continuous variable. display side--side assign plots objects (p1 p2) since loaded patchwork package can simply return two plot objects combining + operator.  Now plot looks fine, can tweak remove legend (“Modality”) left-pane plot (p1) appear twice fill space plotting space. can change underlying theme p1 theme() layer function set argument legend.position = \"none\". can add theme change one two ways. first just add theme(legend.position = \"none\") list layer features create p1 plot second can modify p1 object ’s created calling p1 applying change sending results new object (can name overwrite previous plot object p1). Let’s take second approach demonstration purposes. also want show can create annotations patchwork add general plot title subtitle whole plotting space. use plot_annotation() function combining p1 p2.  Now let’s interpret visualization. appears prediction support. length recipient longer dative realized ‘PP’ (left pane) length theme longer recipient realized ‘NP’ (right pane). also see appears trend written language longer themes recipients overall. intuitions mind let’s now look submit data statistical test.","code":"dative %>% # dataset   skim() # get data summary dative %>% # dataset   skim() %>% # get data summary   yank(\"factor\") # only show factor-oriented information num_skim <- skim_with(numeric = sfl(iqr = IQR)) # add IQR to custom skim  dative %>%    num_skim() %>% # get custom data summary   yank(\"numeric\") # only show numeric-oriented information dative %>%    group_by(realization_of_recipient, modality) %>% # group by categorical variables   num_skim() %>% # get custom data summary   yank(\"numeric\") # only show numeric-oriented information p1 <-    dative %>% # dataset   ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings   geom_boxplot() +  # box plot   labs(x = \"Realization of recipient\", y = \"Length of recipient (in words)\", color = \"Modality\") # labels  p2 <-    dative %>% # dataset   ggplot(aes(x = realization_of_recipient, y = length_of_theme, color = modality)) + # mappings   geom_boxplot() +  # boxplot   labs(x = \"Realization of recipient\", y = \"Length of them (in words)\", color = \"Modality\") # labels  p1 + p2 p1 <-    dative %>% # dataset   ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings   geom_boxplot() +  # box plot   labs(x = \"Realization of recipient\", y = \"Length of recipient (in words)\", color = \"Modality\") # labels  p1 <- p1 + theme(legend.position = \"none\") # remove legend from left plot  p2 <-    dative %>% # dataset   ggplot(aes(x = realization_of_recipient, y = length_of_theme, color = modality)) + # mappings   geom_boxplot() +  # boxplot   labs(x = \"Realization of recipient\", y = \"Length of them (in words)\", color = \"Modality\") # labels  p1 + p2 + plot_annotation(title = \"Realization of the recipient\",                            subtitle = \"The relationship between recipient/ theme lengths and modality\")"},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"statistical-interrogation","dir":"Articles","previous_headings":"Coding strategies","what":"Statistical interrogation","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"Since dependent variable realization_of_recipient categorical exactly two levels (‘NP’ ‘PP’) considering two main effects (length_of_recipient length_of_theme) turn regression model. two-level categorical dependent variable use logistic regression type Generalized Linear Model. glm() function provide us mechanism run test. need make sure specify formula analysis family distribution. formula realization_of_recipient ~ length_of_recipient + length_of_theme + modality. Now ’ve included modality model control variable. aim interpret modality effect include model can account amount variability realization_of_recipient otherwise left unaccounted . Control variables often variables known play part relationship, part hypothesis tested. Looking ‘Coefficients’ sub-table, can see main effects found significant (Significance codes helpful guide). Since know nature trends prior numeric visual assessments, can readily interpret meaning results. effect hypothesis found significant.","code":"m1 <- glm(formula = realization_of_recipient ~ length_of_recipient + length_of_theme + modality, # formula           data = dative, # dataset            family = \"binomial\") # distribution family  summary(m1) # preview statistical results ##  ## Call: ## glm(formula = realization_of_recipient ~ length_of_recipient +  ##     length_of_theme + modality, family = \"binomial\", data = dative) ##  ## Deviance Residuals:  ##    Min      1Q  Median      3Q     Max   ## -4.136  -0.658  -0.497   0.065   4.322   ##  ## Coefficients: ##                     Estimate Std. Error z value Pr(>|z|)     ## (Intercept)          -1.5909     0.0917  -17.35  < 2e-16 *** ## length_of_recipient   0.7792     0.0457   17.06  < 2e-16 *** ## length_of_theme      -0.3046     0.0231  -13.21  < 2e-16 *** ## modalityWritten       0.5555     0.1174    4.73  2.2e-06 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 3741.1  on 3262  degrees of freedom ## Residual deviance: 2816.0  on 3259  degrees of freedom ## AIC: 2824 ##  ## Number of Fisher Scoring iterations: 6"},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"evaluation","dir":"Articles","previous_headings":"Coding strategies","what":"Evaluation","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"continue, verify effect sizes reliability test statistic confidence interval. can use effectsize() function effectsize package purpose. effects object data frame can use pass relevant coefficient interpret_r() function provide interpretation strength effect.","code":"effects <- effectsize(m1)  # get test statistics and confidence intervals  effects  # preview test statistic and confidence interval interpret_r(effects$Std_Coefficient[2])  # length of recipient ## [1] \"very large\" ## (Rules: funder2019) interpret_r(effects$Std_Coefficient[3])  # length of theme ## [1] \"very large\" ## (Rules: funder2019)"},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"reporting","dir":"Articles","previous_headings":"Coding strategies","what":"Reporting","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"Reporting results study write-ups important final step communicating findings. report package helpful function automatically create boilerplate set reports including paper. function report_text() used model output, case m1. report package allows us pull pieces information want well. purposes output great start write . may, however, want include table report statistics. can use report_table() function provide table.","code":"report_text(m1) We fitted a logistic model (estimated using ML) to predict realization_of_recipient with length_of_recipient, length_of_theme and modality (formula: realization_of_recipient ~ length_of_recipient + length_of_theme + modality). The model's explanatory power is substantial (Tjur's R2 = 0.30). The model's intercept, corresponding to length_of_recipient = 0, length_of_theme = 0 and modality = Spoken, is at -1.59 (95% CI [-1.77, -1.41], p < .001). Within this model:    - The effect of length of recipient is statistically significant and positive (beta = 0.78, 95% CI [0.69, 0.87], p < .001; Std. beta = 1.61, 95% CI [1.43, 1.80])   - The effect of length of theme is statistically significant and negative (beta = -0.30, 95% CI [-0.35, -0.26], p < .001; Std. beta = -1.33, 95% CI [-1.53, -1.14])   - The effect of modality [Written] is statistically significant and positive (beta = 0.56, 95% CI [0.32, 0.78], p < .001; Std. beta = 0.56, 95% CI [0.32, 0.78])  Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using report_table(m1)  # get the report statistics table"},{"path":"https://lin380.github.io/tadr/articles/recipe_9.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"9. Statistical inference: prep, assess, interrogate, evaluate, and report","text":"Recipe seen number key conceptual programming strategies used inferential data analysis.","code":""},{"path":"https://lin380.github.io/tadr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jerid Francom. Author, maintainer.","code":""},{"path":"https://lin380.github.io/tadr/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Francom J (2022). tadr: Text Data Resources. R package version 0.1.1, https://github.com/lin380/tadr.","code":"@Manual{,   title = {tadr: Text as Data Resources},   author = {Jerid Francom},   year = {2022},   note = {R package version 0.1.1},   url = {https://github.com/lin380/tadr}, }"},{"path":"https://lin380.github.io/tadr/index.html","id":"tadr","dir":"","previous_headings":"","what":"Text as Data Resources","title":"Text as Data Resources","text":"goal tadr provide supporting resources coursebook “Text Data: introduction quantitative text analysis reproducible research R”.","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/index.html","id":"installation","dir":"","previous_headings":"tadr package","what":"Installation","title":"Text as Data Resources","text":"can install development version tadr GitHub :","code":"install.packages(\"devtools\") devtools::install_github(\"lin380/tadr\")"},{"path":"https://lin380.github.io/tadr/index.html","id":"load","dir":"","previous_headings":"tadr package","what":"Load","title":"Text as Data Resources","text":"load package :","code":"library(tadr)"},{"path":"https://lin380.github.io/tadr/reference/brown.html","id":null,"dir":"Reference","previous_headings":"","what":"Brown Corpus — brown","title":"Brown Corpus — brown","text":"dataset containing 1,155,866 tokenized words 15 genre categories sample American English.","code":""},{"path":"https://lin380.github.io/tadr/reference/brown.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Brown Corpus — brown","text":"","code":"brown"},{"path":"https://lin380.github.io/tadr/reference/brown.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Brown Corpus — brown","text":"data frame 223,506 rows 11 variables: document_id ID corpus document category Label code 15 corpus categories category_description Description label corpus categories words Tokenized words corpus pos Part speech label word corpus","code":""},{"path":"https://lin380.github.io/tadr/reference/brown.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Brown Corpus — brown","text":"http://www.nltk.org/nltk_data/","code":""},{"path":"https://lin380.github.io/tadr/reference/europarle_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","title":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","text":"Europarl Parallel Corpus extracted proceedings European Parliament. corpus sample Spanish-English pair.","code":""},{"path":"https://lin380.github.io/tadr/reference/europarle_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","text":"","code":"data(\"europarle_sample\")"},{"path":"https://lin380.github.io/tadr/reference/europarle_sample.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","text":"data frame 200,000 observations following 3 variables. type Either: Source Target language sentence_id Id index sentence pairs sentence line proceedings, including comments","code":""},{"path":"https://lin380.github.io/tadr/reference/europarle_sample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","text":"Version 7 release.","code":""},{"path":"https://lin380.github.io/tadr/reference/europarle_sample.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","text":"https://www.statmt.org/europarl/","code":""},{"path":"https://lin380.github.io/tadr/reference/europarle_sample.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","text":"Koehn, P. (2005, September). Europarl: parallel corpus statistical machine translation. MT summit (Vol. 5, pp. 79-86).","code":""},{"path":"https://lin380.github.io/tadr/reference/europarle_sample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"European Parliament Proceedings Parallel Corpus 1996-2011 (Spanish-English) — europarle_sample","text":"","code":"data(europarle_sample)"},{"path":"https://lin380.github.io/tadr/reference/get_compressed_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a Compressed File and Decompress its Contents — get_compressed_data","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"Possible file types include .zip, .gz, .tar","code":""},{"path":"https://lin380.github.io/tadr/reference/get_compressed_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"","code":"get_compressed_data(url, target_dir, force = FALSE)"},{"path":"https://lin380.github.io/tadr/reference/get_compressed_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"url character vector representing full url compressed file target_dir directory compressed file downloaded force optional argument forcefully overwrites existing data","code":""},{"path":"https://lin380.github.io/tadr/reference/get_compressed_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"Download extract compressed data file","code":""},{"path":"https://lin380.github.io/tadr/reference/get_compressed_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download a Compressed File and Decompress its Contents — get_compressed_data","text":"","code":"get_compressed_data(url = \"http://www.test.com/file.zip\", target_dir = \"./\") #> Data already exists  if (FALSE) { get_compressed_data(url = \"http://www.test.com/file.zip\", target_dir = \"./\") }"},{"path":"https://lin380.github.io/tadr/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://lin380.github.io/tadr/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://lin380.github.io/tadr/reference/sms.html","id":null,"dir":"Reference","previous_headings":"","what":"SMS Spam Collection v. 1 — sms","title":"SMS Spam Collection v. 1 — sms","text":"SMS Spam Collection v.1 public set SMS labeled messages collected mobile phone spam research. one collection composed 5,574 English, real non-enconded messages, tagged according legitimate (ham) spam.","code":""},{"path":"https://lin380.github.io/tadr/reference/sms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SMS Spam Collection v. 1 — sms","text":"","code":"data(\"sms\")"},{"path":"https://lin380.github.io/tadr/reference/sms.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"SMS Spam Collection v. 1 — sms","text":"data frame 5,574 observations following 2 variables. sms_type Type message; either spam ham message sms message","code":""},{"path":[]},{"path":"https://lin380.github.io/tadr/reference/sms.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"SMS Spam Collection v. 1 — sms","text":"http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip","code":""},{"path":"https://lin380.github.io/tadr/reference/sms.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SMS Spam Collection v. 1 — sms","text":"http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/","code":""},{"path":"https://lin380.github.io/tadr/reference/sms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SMS Spam Collection v. 1 — sms","text":"","code":"data(sms)"},{"path":"https://lin380.github.io/tadr/reference/swda.html","id":null,"dir":"Reference","previous_headings":"","what":"Switchboard Dialog Act Corpus — swda","title":"Switchboard Dialog Act Corpus — swda","text":"dataset containing 1,155 5-minute conversations 441 speakers American English created 1997 tagged shallow discourse tagset approximately 60 basic dialog act tags (DAMSL) combinations.","code":""},{"path":"https://lin380.github.io/tadr/reference/swda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Switchboard Dialog Act Corpus — swda","text":"","code":"data(\"swda\")"},{"path":"https://lin380.github.io/tadr/reference/swda.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Switchboard Dialog Act Corpus — swda","text":"data frame 223,606 observations following 16 variables. doc_id ID conversation document topic_num Topic number associated conversation topicality Subjective rating annotator whether callers conversed generally suggested recorded prompt. Scale 1 5, 1 topic. naturalness Subjective rating annotator whether conversation sounded natural. Scale 1 5, 1 natural. damsl_tag DAMSL dialog act annotation labels speaker Label speaker conversation turn_num Number contiguous utterance turns given speaker utterance_num cumulative number utterances conversation utterance_text actual dialog utterance. Includes disfluency annotation (see details ) speaker_id ID speaker sex biological sex speaker birth_year Year speaker born dialect_area Region US speaker spent first 10 years education Highest educational level attained: values 0, 1, 2, 3, 9 topic Topic description topic_prompt Specific topic prompt conversation","code":""},{"path":"https://lin380.github.io/tadr/reference/swda.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Switchboard Dialog Act Corpus — swda","text":"information metadata data can found : https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_manual.txt. SWBD-DAMSL manual can found : https://web.stanford.edu/~jurafsky/ws97/manual.august1.html. Dysfluency Annotation Stylebook Switchboard Corpus can found : https://staff.fnwi.uva.nl/r.fernandezrovira/teaching/DM-materials/DFL-book.pdf.","code":""},{"path":"https://lin380.github.io/tadr/reference/swda.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Switchboard Dialog Act Corpus — swda","text":"Switchboard-1 Release 2 https://catalog.ldc.upenn.edu/docs/LDC97S62/","code":""},{"path":"https://lin380.github.io/tadr/reference/swda.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Switchboard Dialog Act Corpus — swda","text":"Godfrey, John J., Edward Holliman. Switchboard-1 Release 2 LDC97S62. Web Download. Philadelphia: Linguistic Data Consortium, 1993. Jurafsky, Daniel, Elizabeth Shriberg, Debra Biasca. 1997. \"Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual, Draft 13\"  University Colorado, Boulder Institute Cognitive Science Technical Report 97-02 Meteer, Marie Ann Taylor. 1995.  Dysfluency Annotation Stylebook Switchboard Corpus","code":""},{"path":"https://lin380.github.io/tadr/reference/swda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Switchboard Dialog Act Corpus — swda","text":"","code":"data(swda)"}]
